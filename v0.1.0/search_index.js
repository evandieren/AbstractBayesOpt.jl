var documenterSearchIndex = {"docs":
[{"location":"tutorials/acq_funcs_comparison/#AbstractBayesOpt-Tutorial:-Acquisition-Functions-Comparison-with-gradient-enhanced-GPs","page":"1D Acquisition Function Comparison","title":"AbstractBayesOpt Tutorial: Acquisition Functions Comparison with gradient-enhanced GPs","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Setup","page":"1D Acquisition Function Comparison","title":"Setup","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Loading the necessary packages.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"using AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(555) # hide\nnothing # hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Define-the-objective-function","page":"1D Acquisition Function Comparison","title":"Define the objective function","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"We will compare different acquisition functions on a 1D function with multiple local minima: f(x) = sin(x + 1) + sin(frac103(x + 1))","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"f(x) = sin(x + 1) + sin((10.0 / 3.0) * (x + 1))\n∂f(x) = ForwardDiff.derivative(f, x)\nmin_f = −1.988699758534924 # hide\nf_∂f(x) = [f(x); ∂f(x)];\n\nd = 1\nlower = [-10.0]\nupper = [10.0]\ndomain = ContinuousDomain(lower, upper)\n\nplot_domain = lower[1]:0.01:upper[1] # hide\nys = f.(plot_domain) # hide\n\nplot( # hide\n    plot_domain, # hide\n    ys; # hide\n    xlim=(lower[1], upper[1]), # hide\n    label=\"f(x)\", # hide\n    xlabel=\"x\", # hide\n    ylabel=\"f(x)\", # hide\n    title=\"Objective Function: sin(x+1) + sin(10(x+1)/3)\", # hide\n    legend=:outertopright, # hide\n) # hide\n\napprox_min = [-8.9981; 9.8514] # hide\nf_min = f.(approx_min) # hide\nscatter!([approx_min], [f_min]; label=\"Global minima\", color=:red, markersize=5) # hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Initial-Training-Data","page":"1D Acquisition Function Comparison","title":"Initial Training Data","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"We will use a gradient-enhanced Gaussian Process (GradientGP) with a Matérn 5/2 kernel. We add a small noise variance for numerical stability.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"σ² = 1e-12;\nnothing #hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Generate initial training data using Sobol sampling for better space coverage","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"n_train = 5\nx_train = vec(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\ny_train = f_∂f.(x_train)","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Setup the gradient-enhanced GP model, using in-house ApproxMatern52Kernel for AD compatibility.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"model = GradientGP(ApproxMatern52Kernel(), d+1, σ²)","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Acquisition-Functions-Setup","page":"1D Acquisition Function Comparison","title":"Acquisition Functions Setup","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"We will compare five different acquisition functions:","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Expected Improvement (EI): Balances exploitation and exploration by considering both the magnitude and probability of improvement (see ExpectedImprovement)\nProbability of Improvement (PI): Focuses on the probability of finding a better point (see ProbabilityImprovement)\nUpper Confidence Bound (UCB): Uses lower bound estimates to guide exploration (see UpperConfidenceBound)\nUCB + Gradient UCB Ensemble: Combines standard UCB with gradient norm information (see GradientNormUCB and EnsembleAcquisition)\nEI + Gradient UCB Ensemble: Combines Expected Improvement with gradient norm information (see GradientNormUCB and EnsembleAcquisition)","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"We show below the function to setup the acquisition functions, and run the tests. You can skip to the results analysis and visualisation section if you want to see the outcomes directly.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"function setup_acquisition_functions(y_train)\n    best_y = minimum(first.(y_train))\n\n    ei_acq = ExpectedImprovement(0.0, best_y)\n\n    pi_acq = ProbabilityImprovement(0.0, best_y)\n\n    ucb_acq = UpperConfidenceBound(1.96)\n\n    ucb_acq_for_ensemble = UpperConfidenceBound(1.96)\n    grad_ucb_acq = GradientNormUCB(1.5)\n    ensemble_ucb_grad = EnsembleAcquisition(\n        [0.9, 0.1], [ucb_acq_for_ensemble, grad_ucb_acq]\n    )\n\n    ei_for_ensemble = ExpectedImprovement(0.0, best_y)\n    grad_ucb_for_ensemble = GradientNormUCB(1.5)\n    ensemble_ei_grad = EnsembleAcquisition(\n        [0.9, 0.1], [ei_for_ensemble, grad_ucb_for_ensemble]\n    )\n\n    return [\n        (\"EI\", ei_acq),\n        (\"PI\", pi_acq),\n        (\"UCB\", ucb_acq),\n        (\"UCB+GradUCB\", ensemble_ucb_grad),\n        (\"EI+GradUCB\", ensemble_ei_grad),\n    ]\nend","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Running-the-Optimisation-Comparison","page":"1D Acquisition Function Comparison","title":"Running the Optimisation Comparison","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Now we will run Bayesian optimisation with each acquisition function and compare their performance.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"function run_comparison(n_iterations=30)\n    results = Dict{String,Any}()\n\n    for (name, acq_func) in setup_acquisition_functions(y_train)\n        @info \"\\n=== Running optimisation with $name ===\"\n\n        problem = BOStruct(\n            f_∂f,\n            acq_func,\n            model,\n            domain,\n            x_train,\n            y_train,\n            n_iterations,\n            0.0,  # Actual noise level (0.0 for noiseless)\n        )\n\n        result, _, _ = AbstractBayesOpt.optimize(problem);\n\n        xs = result.xs\n        ys = first.(result.ys_non_std)\n\n        optimal_point = xs[argmin(ys)]\n        optimal_value = minimum(ys)\n\n        @info \"Optimal point: $optimal_point\"\n        @info \"Optimal value: $optimal_value\"\n        @info \"Error from true minimum: $(abs(optimal_value - min_f))\"\n\n        running_min = accumulate(min, f.(xs));\n\n        results[name] = (\n            xs=xs,\n            ys=ys,\n            running_min=running_min,\n            optimal_point=optimal_point,\n            optimal_value=optimal_value,\n            error=abs(optimal_value - min_f),\n        )\n    end\n\n    return results\nend;\nnothing #hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Execute-the-comparison","page":"1D Acquisition Function Comparison","title":"Execute the comparison","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"Let's run the optimisation with each acquisition function for 30 iterations.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"@info \"Starting acquisition function comparison...\" # hide\nresults = run_comparison(30)","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Results-Analysis-and-Visualisation","page":"1D Acquisition Function Comparison","title":"Results Analysis and Visualisation","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"We will create a convergence plot showing how each acquisition function performs over time. The plot shows the error relative to the true minimum on a logarithmic scale.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"function plot_convergence(results) # hide\n    p = plot(; # hide\n        title=\"Acquisition Function Comparison (1D GradBO)\", # hide\n        xlabel=\"Function evaluations\", # hide\n        yaxis=:log, # hide\n        legend=:bottomleft, # hide\n        linewidth=2, # hide\n    ) # hide\n\n    colors = [:blue, :red, :green, :orange, :purple] # hide\n\n    for (i, (name, result)) in enumerate(results) # hide\n        running_min_extended = collect( # hide\n            Iterators.flatten(fill(x, 2) for x in result.running_min), # hide\n        ) # hide\n        errors = max.(running_min_extended .- min_f, 1e-16)  # Avoid log(0) # hide\n\n        plot!( # hide\n            p, # hide\n            (2 * n_train):length(errors), # hide\n            errors[(2 * n_train):end]; # hide\n            label=name, # hide\n            color=colors[i], # hide\n            alpha=0.8, # hide\n        ) # hide\n    end # hide\n\n    vspan!(p, [1, 2*n_train]; color=:gray, alpha=0.2, label=\"Initial data\") # hide\n\n    return p # hide\nend; # hide\nnothing #hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/#Create-and-display-the-convergence-plot","page":"1D Acquisition Function Comparison","title":"Create and display the convergence plot","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"This plot shows how quickly each acquisition function converges to the global minimum. The ensemble methods that combine multiple acquisition functions often show improved performance.","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"conv_plot = plot_convergence(results) # hide","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"","category":"page"},{"location":"tutorials/acq_funcs_comparison/","page":"1D Acquisition Function Comparison","title":"1D Acquisition Function Comparison","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#AbstractBayesOpt-Tutorial:-NLML-Landscape-Visualisation","page":"Log Likelihood Landscape Visualisation","title":"AbstractBayesOpt Tutorial: NLML Landscape Visualisation","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"This tutorial shows how to visualise the negative log marginal likelihood (NLML) landscape for Gaussian Process models. The NLML landscape shows how the model likelihood changes as we vary the kernel hyperparameters. Understanding this landscape is crucial for:","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Choosing appropriate hyperparameter optimisation strategies\nUnderstanding why some configurations converge faster than others\nIdentifying potential issues like local minima or ill-conditioned regions","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Setup","page":"Log Likelihood Landscape Visualisation","title":"Setup","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Loading the necessary packages.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"using AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(42) # hide\nnothing # hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Define-the-objective-function","page":"Log Likelihood Landscape Visualisation","title":"Define the objective function","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"We'll use the Himmelblau function again, as it provides a good test case with complex structure.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"himmelblau(x::AbstractVector) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n∇himmelblau(x::AbstractVector) = ForwardDiff.gradient(himmelblau, x)\nf_val_grad(x::AbstractVector) = [himmelblau(x); ∇himmelblau(x)];\nnothing #hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Problem-Setup-and-Training-Data","page":"Log Likelihood Landscape Visualisation","title":"Problem Setup and Training Data","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"d = 2\nlower = [-4.0, -4.0]\nupper = [4.0, 4.0]\ndomain = ContinuousDomain(lower, upper)\nσ² = 1e-6","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Generate training data using Sobol sampling for better coverage","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"n_train = 75\nx_train = [\n    collect(col) for\n    col in eachcol(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\n]","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Evaluate function at training points for both model types","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"y_train_standard = [himmelblau(x) for x in x_train]  # Standard GP: only function values\ny_train_gradient = f_val_grad.(x_train);    # Gradient GP: function values + gradients\nnothing #hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Setup-Gaussian-Process-Models","page":"Log Likelihood Landscape Visualisation","title":"Setup Gaussian Process Models","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"We'll create both standard and gradient-enhanced GP models using the same kernel type but configured for their respective data structures.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"kernel = ApproxMatern52Kernel()\n\nstandard_model = StandardGP(kernel, σ²)\ngradient_model = GradientGP(kernel, d+1, σ²)","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Prepare data for NLML computation (this is done under the hood in AbstractBayesOpt.jl)","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Standard GP data structure","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"x_standard = x_train;\ny_standard = reduce(vcat, y_train_standard);\nnothing #hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Gradient GP data structure","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"x_gradient = KernelFunctions.MOInputIsotopicByOutputs(x_train, d+1);\ny_gradient = vec(permutedims(reduce(hcat, y_train_gradient)));\n\nprintln(\"Data shapes for NLML computation:\") # hide\nprintln(\"  Standard GP: x=$(length(x_standard)), y=$(length(y_standard))\") # hide\nprintln(\"  Gradient GP: x=$(length(x_gradient)), y=$(length(y_gradient))\") # hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Define-Parameter-Ranges-for-NLML-Landscape","page":"Log Likelihood Landscape Visualisation","title":"Define Parameter Ranges for NLML Landscape","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"We will create a grid of hyperparameter values to evaluate the NLML landscape. The parameters we will vary are:","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"Length scale: Controls how quickly the function varies spatially\nScale parameter: Controls the overall magnitude of function variations","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"log_lengthscale_range = range(log(1e-3), log(1e3); length=100) # hide\nlog_scale_range = range(log(1e-3), log(1e6); length=100) # hide\n\nprintln(\"Parameter ranges:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(exp(log_lengthscale_range[1]), digits=3)) to $(round(exp(log_lengthscale_range[end]), digits=3))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(exp(log_scale_range[1]), digits=3)) to $(round(exp(log_scale_range[end]), digits=3))\", # hide\n) # hide\n\nfunction compute_nlml_landscape( # hide\n    model,\n    x_data,\n    y_data,\n    log_ls_range,\n    log_scale_range,\n    model_name, # hide\n) # hide\n    @info \"Computing NLML landscape for $model_name...\" # hide\n\n    nlml_values = zeros(length(log_ls_range), length(log_scale_range)) # hide\n\n    total_combinations = length(log_ls_range) * length(log_scale_range) # hide\n    completed = 0 # hide\n\n    for (i, log_ls) in enumerate(log_ls_range) # hide\n        for (j, log_scale) in enumerate(log_scale_range) # hide\n            try # hide\n                params = [log_ls, log_scale] # hide\n                nlml_val = AbstractBayesOpt.nlml(model, params, x_data, y_data) # hide\n                nlml_values[i, j] = nlml_val # hide\n\n                if !isfinite(nlml_val) || nlml_val > 1e9 # hide\n                    @warn \"Warning: NLML value out of bounds at (log_ls=$(round(log_ls, digits=2)), log_scale=$(round(log_scale, digits=2))): $nlml_val\" # hide\n                    nlml_values[i, j] = 1e9 # hide\n                end # hide\n            catch e # hide\n                nlml_values[i, j] = 1e9 # hide\n            end # hide\n\n            completed += 1 # hide\n            if completed % 500 == 0 # hide\n                progress = round(100 * completed / total_combinations; digits=1) # hide\n                @info \"  Progress: $progress% ($completed/$total_combinations)\" # hide\n            end # hide\n        end # hide\n    end # hide\n\n    @info \"  Completed NLML landscape computation for $model_name\" # hide\n    @info \"  NLML range: $(round(minimum(nlml_values), digits=2)) to $(round(maximum(nlml_values), digits=2))\" # hide\n\n    return nlml_values # hide\nend # hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Compute-landscapes-for-both-models","page":"Log Likelihood Landscape Visualisation","title":"Compute landscapes for both models","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"This computation may take several minutes depending on the grid resolution. We're evaluating 10,000 parameter combinations for each model type.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"nlml_standard = compute_nlml_landscape(\n    standard_model,\n    x_standard,\n    y_standard,\n    log_lengthscale_range,\n    log_scale_range,\n    \"Standard GP\",\n)\n\nnlml_gradient = compute_nlml_landscape(\n    gradient_model,\n    x_gradient,\n    y_gradient,\n    log_lengthscale_range,\n    log_scale_range,\n    \"Gradient GP\",\n)","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"This provides a 100x100 grid of NLML values for each model type.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#Optimal-Parameters","page":"Log Likelihood Landscape Visualisation","title":"Optimal Parameters","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"We approximately provide the hyperparameter combinations that minimise the NLML for each model type. In AbstractBayesOpt.jl, we optimise the MLML using Optim.jl's BFGS method.","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"function find_optimal_params(nlml_values, log_ls_range, log_scale_range) # hide\n    min_idx = argmin(nlml_values) # hide\n    i, j = Tuple(min_idx) # hide\n    opt_log_ls = log_ls_range[i] # hide\n    opt_log_scale = log_scale_range[j] # hide\n    opt_nlml = nlml_values[i, j] # hide\n\n    return ( # hide\n        log_lengthscale=opt_log_ls, # hide\n        log_scale=opt_log_scale, # hide\n        lengthscale=exp(opt_log_ls), # hide\n        scale=exp(opt_log_scale), # hide\n        nlml=opt_nlml, # hide\n    ) # hide\nend # hide\n\nopt_standard = find_optimal_params(nlml_standard, log_lengthscale_range, log_scale_range) # hide\nopt_gradient = find_optimal_params(nlml_gradient, log_lengthscale_range, log_scale_range) # hide\n\nprintln(\"\\nOptimal parameters found:\") # hide\nprintln(\"Standard GP:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(opt_standard.lengthscale, digits=3)) (log: $(round(opt_standard.log_lengthscale, digits=3)))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(opt_standard.scale, digits=3)) (log: $(round(opt_standard.log_scale, digits=3)))\", # hide\n) # hide\nprintln(\"  NLML: $(round(opt_standard.nlml, digits=3))\") # hide\n\nprintln(\"\\nGradient GP:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(opt_gradient.lengthscale, digits=3)) (log: $(round(opt_gradient.log_lengthscale, digits=3)))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(opt_gradient.scale, digits=3)) (log: $(round(opt_gradient.log_scale, digits=3)))\", # hide\n) # hide\nprintln(\"  NLML: $(round(opt_gradient.nlml, digits=3))\") # hide\n\nfunction create_contour_plot( # hide\n    nlml_values,\n    log_ls_range,\n    log_scale_range,\n    title_str,\n    optimal_params=nothing, # hide\n) # hide\n    p = contourf( # hide\n        log_scale_range, # hide\n        log_ls_range, # hide\n        log.(nlml_values); # hide\n        title=title_str, # hide\n        xlabel=\"log Scale Parameter\", # hide\n        ylabel=\"log Lengthscale Parameter\", # hide\n        color=:coolwarm, # hide\n        fill=true, # hide\n        levels=50, # hide\n        aspect_ratio=:equal, # hide\n        size=(600, 500), # hide\n    ) # hide\n\n    if optimal_params !== nothing # hide\n        scatter!( # hide\n            p, # hide\n            [log.(optimal_params.scale)], # hide\n            [log.(optimal_params.lengthscale)]; # hide\n            color=:red, # hide\n            markersize=8, # hide\n            markershape=:star, # hide\n            label=\"Optimal\", # hide\n            legend=:bottomright,\n        ) # hide\n    end # hide\n\n    return p # hide\nend; # hide\nnothing #hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/#NLML-Landscape-Plots","page":"Log Likelihood Landscape Visualisation","title":"NLML Landscape Plots","text":"","category":"section"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"These contour plots show the NLML landscape for both model types. The star indicates the optimal hyperparameter combination found through the approximate minimisers over the 100x100 grid. Darker blue regions correspond to lower NLML values (better likelihood).","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"contour_standard = create_contour_plot( # hide\n    nlml_standard, # hide\n    log_lengthscale_range, # hide\n    log_scale_range, # hide\n    \"Standard GP log(NLML) Contours\", # hide\n    opt_standard, # hide\n) # hide\n\ncontour_gradient = create_contour_plot( # hide\n    nlml_gradient, # hide\n    log_lengthscale_range, # hide\n    log_scale_range, # hide\n    \"Gradient GP log(NLML) Contours\", # hide\n    opt_gradient, # hide\n) # hide\n\ncombined_contours = plot( # hide\n    contour_standard, # hide\n    contour_gradient; # hide\n    layout=(1, 2), # hide\n    size=(1200, 500), # hide\n    plot_title=\"NLML Contour Comparison: Standard vs Gradient GP\", # hide\n) # hide","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"","category":"page"},{"location":"tutorials/nlml_landscape_visualization/","page":"Log Likelihood Landscape Visualisation","title":"Log Likelihood Landscape Visualisation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/2D_BO/#AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation","page":"2D Bayesian Optimisation","title":"AbstractBayesOpt Tutorial: Basic 2D Optimisation","text":"","category":"section"},{"location":"tutorials/2D_BO/#Setup","page":"2D Bayesian Optimisation","title":"Setup","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Loading the necessary packages.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"using AbstractBayesOpt\nusing AbstractGPs\nusing ForwardDiff\nusing Plots\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nusing Random # hide\nRandom.seed!(42) # hide\nnothing #hide","category":"page"},{"location":"tutorials/2D_BO/#Define-the-objective-function","page":"2D Bayesian Optimisation","title":"Define the objective function","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"f(x) = (x[1]^2 + x[2] - 11)^2 + (x[1]+x[2]^2-7)^2\nmin_f = 0.0 # hide\nd = 2\ndomain = ContinuousDomain([-6.0, -6.0], [6.0, 6.0])\n\nresolution = 100 #hide\nX = range(domain.lower[1], domain.upper[1]; length=resolution) # hide\nY = range(domain.lower[2], domain.upper[2]; length=resolution) # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> f([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Target function : Himmelblau\", # hide\n    xlabel=\"x₁\", # hide\n    ylabel=\"x₂\", # hide\n) # hide\n\nx_mins = [[3.0, 2.0], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]] #hide","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Scatter them on the contour plot","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"scatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"Minima\", # hide\n    color=:red, # hide\n    markersize=5, # hide\n    legend=:bottomright, # hide\n) # hide","category":"page"},{"location":"tutorials/2D_BO/#Standard-GPs","page":"2D Bayesian Optimisation","title":"Standard GPs","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"We'll use a standard Gaussian Process surrogate with a squared-exponential kernel. We add a small jitter term for numerical stability of 10^-9.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"noise_var = 1e-9\nsurrogate = StandardGP(SqExponentialKernel(), noise_var)","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Generate uniform random samples x_train","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"n_train = 5\nx_train = [domain.lower .+ (domain.upper .- domain.lower) .* rand(d) for _ in 1:n_train]\n\ny_train = f.(x_train)","category":"page"},{"location":"tutorials/2D_BO/#Choose-an-acquisition-function","page":"2D Bayesian Optimisation","title":"Choose an acquisition function","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"We'll use the Expected Improvement acquisition function with an exploration parameter ξ = 0.0.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"ξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))","category":"page"},{"location":"tutorials/2D_BO/#Set-up-the-Bayesian-Optimisation-structure","page":"2D Bayesian Optimisation","title":"Set up the Bayesian Optimisation structure","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"We use BOStruct to bundle all components needed for the optimization. Here, we set the number of iterations to 5 and the actual noise level to 0.0 (since our function is noiseless). We then run the optimize function to perform the Bayesian Optimisation.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"bo_struct = BOStruct(\n    f,\n    acq,\n    surrogate,\n    domain,\n    x_train,\n    y_train,\n    50,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\"\nresult, acq_list, standard_params = AbstractBayesOpt.optimize(\n    bo_struct; standardize=\"mean_only\"\n);\nnothing #hide","category":"page"},{"location":"tutorials/2D_BO/#Results","page":"2D Bayesian Optimisation","title":"Results","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"The optimization result is stored in result. We can print the best found input and its corresponding function value.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"xs = result.xs # hide\nys = result.ys_non_std # hide\n\nprintln(\"Optimal point: \", xs[argmin(ys)]) # hide\nprintln(\"Optimal value: \", minimum(ys)) # hide","category":"page"},{"location":"tutorials/2D_BO/#Plotting-of-running-minimum-over-iterations","page":"2D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"The running minimum is the best function value found up to each iteration.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"running_min = accumulate(min, f.(xs)) # hide\n\np = Plots.plot( # hide\n    n_train:length(running_min), # hide\n    running_min[n_train:end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (2D BO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"BO\", # hide\n    xlims=(1, length(running_min)), # hide\n) # hide\nPlots.vspan!([1, n_train]; color=:blue, alpha=0.2, label=\"\") # hide","category":"page"},{"location":"tutorials/2D_BO/#Gradient-enhanced-GPs","page":"2D Bayesian Optimisation","title":"Gradient-enhanced GPs","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Now, let's see how to use gradient information to improve the optimization. We'll use the same function but now also provide its gradient. We define a new surrogate model that can handle gradient information, specifically a GradientGP.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"grad_surrogate = GradientGP(SqExponentialKernel(), d + 1, noise_var)\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))\n\n∇f(x) = ForwardDiff.gradient(f, x)\nf_val_grad(x) = [f(x); ∇f(x)];\nnothing #hide","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Generate value and gradients at random samples","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"y_train_grad = f_val_grad.(x_train)","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"Set up the Bayesian Optimisation structure","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"bo_struct_grad = BOStruct(\n    f_val_grad,\n    acq,\n    grad_surrogate,\n    domain,\n    x_train,\n    y_train_grad,\n    20,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\" # hide\nresult_grad, acq_list_grad, standard_params_grad = AbstractBayesOpt.optimize(bo_struct_grad);\nnothing #hide","category":"page"},{"location":"tutorials/2D_BO/#Results-2","page":"2D Bayesian Optimisation","title":"Results","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"The optimization result is stored in result_grad. We can print the best found input and its corresponding function value.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"xs_grad = result_grad.xs # hide\nys_grad = first.(result_grad.ys_non_std) # hide\n\nx_min_grad = xs_grad[argmin(ys_grad)] # hide\ny_min_grad = minimum(ys_grad) # hide\n\nprintln(\"Optimal point (GradBO): \", x_min_grad) # hide\nprintln(\"Optimal value (GradBO): \", y_min_grad) # hide","category":"page"},{"location":"tutorials/2D_BO/#Plotting-of-running-minimum-over-iterations-2","page":"2D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"The running minimum is the best function value found up to each iteration. Since each evaluation provides both a function value and a 2D gradient, we duplicate the running minimum values 3x to reflect the number of function evaluations.","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"running_min_grad = accumulate(min, f.(xs_grad)); # hide\nrunning_min_grad = collect(Iterators.flatten(fill(x, 3) for x in (running_min_grad))) # hide\n\np = Plots.plot( # hide\n    (3 * n_train):length(running_min_grad), # hide\n    running_min_grad[(3 * n_train):end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (2D GradBO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"gradBO\", # hide\n    xlims=(1, length(running_min_grad)), # hide\n) # hide\nPlots.vspan!([1, 3 * n_train]; color=:blue, alpha=0.2, label=\"\") # hide","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"We observe that the gradient information does not necessarily lead to a better optimisation path in terms of function evaluations.","category":"page"},{"location":"tutorials/2D_BO/#Plotting-the-surrogate-model","page":"2D Bayesian Optimisation","title":"Plotting the surrogate model","text":"","category":"section"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"We can visualize the surrogate model's mean and uncertainty along with the true function and the evaluated","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"zipped_grid = [ # hide\n    [x1, x2] for # hide\n    (x1, x2) in zip(vec(repeat(X', resolution, 1)), vec(repeat(Y, 1, resolution))) # hide\n] # hide\n\nμ, σ² = unstandardized_mean_and_var(result_grad.model, zipped_grid, standard_params_grad) # hide\nμ_function_grid = reshape(μ[:, 1], resolution, resolution) # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> f([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Target function : Himmelblau\", # hide\n) # hide\n\nscatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"\", # hide\n    color=:green, # hide\n    markershape=:diamond, # hide\n    markersize=5, # hide\n) # hide\n\np2 = contour( # hide\n    X, # hide\n    Y, # hide\n    μ_function_grid; # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Surrogate mean - GradBO\", # hide\n) # hide\nscatter!( # hide\n    p2, # hide\n    [x[1] for x in x_train], # hide\n    [x[2] for x in x_train]; # hide\n    label=\"\", # hide\n    color=:black, # hide\n    markershape=:x, # hide\n    markersize=5,# hide\n) # hide\n\nscatter!( # hide\n    p2, # hide\n    [x[1] for x in xs_grad[(n_train + 1):end]], # hide\n    [x[2] for x in xs_grad[(n_train + 1):end]]; # hide\n    label=\"\", # hide\n    color=:orange, # hide\n    markersize=5, # hide\n) # hide\nscatter!( # hide\n    p2, # hide\n    [x_min_grad[1]], # hide\n    [x_min_grad[2]]; # hide\n    label=\"\", # hide\n    color=:red, # hide\n    markershape=:star5, # hide\n    markersize=8, # hide\n) # hide\n\np_legend = plot(; legend=:bottom, grid=false, axis=false, ticks=false, legend_columns=4) # hide\n\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Training points\", # hide\n    color=:black, # hide\n    markershape=:x, # hide\n    markersize=5, # hide\n) # hide\nscatter!(p_legend, [NaN], [NaN]; label=\"Candidate points\", color=:orange, markersize=5) # hide\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Best candidate\", # hide\n    color=:red, # hide\n    markershape=:star5, # hide\n    markersize=8, # hide\n) # hide\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Function minima\", # hide\n    color=:green, # hide\n    markershape=:diamond, # hide\n    markersize=5, # hide\n) # hide\n\np = plot(p1, p2) # hide\nfinalplot = plot(p, p_legend; layout=@layout([a{0.9h}; b{0.1h}]), size=(1000, 400)) # hide","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"","category":"page"},{"location":"tutorials/2D_BO/","page":"2D Bayesian Optimisation","title":"2D Bayesian Optimisation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference-(Public-API)","page":"Public API","title":"Reference (Public API)","text":"","category":"section"},{"location":"reference/","page":"Public API","title":"Public API","text":"This section documents the exported functions, types, etc from AbstractBayesOpt.jl.","category":"page"},{"location":"reference/","page":"Public API","title":"Public API","text":"","category":"page"},{"location":"reference/#Bayesian-Optimisation-loop","page":"Public API","title":"Bayesian Optimisation loop","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.BOStruct","page":"Public API","title":"AbstractBayesOpt.BOStruct","text":"BOStruct{F,M<:AbstractSurrogate,A<:AbstractAcquisition,D<:AbstractDomain,X,Y,T}(\n    func::F,\n    acq::A,\n    model::M,\n    domain::D,\n    xs::Vector{X},\n    ys::Vector{Y},\n    ys_non_std::Vector{Y},\n    max_iter::Int,\n    iter::Int,\n    noise::T,\n    flag::Bool,\n)\n\nA structure to hold all components of the Bayesian Optimization problem.\n\nAttributes:\n\nfunc::F: The target function to be optimized.\nacq::A: The acquisition function guiding the optimization.\nmodel::M: The surrogate model (e.g., Gaussian Process).\ndomain::D: The domain over which to optimize.\nxs::Vector{X}: A vector of input training points.\nys::Vector{Y}: A vector of corresponding output training values.\nys_non_std::Vector{Y}: A vector of output training values before standardization.\nmax_iter::Int: Maximum number of iterations for the optimization.\niter::Int: Current iteration number.\nnoise::T: Noise level in the observations.\nflag::Bool: A flag to indicate if the optimization should stop due to issues like   ill-conditioning.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.optimize","page":"Public API","title":"AbstractBayesOpt.optimize","text":"optimize(\n    BO::BOStruct;\n    standardize::Union{String,Nothing}=\"mean_scale\",\n    hyper_params::Union{String,Nothing}=\"all\",\n    num_restarts_HP::Int=1,\n)\n\nThis function implements the EGO framework:     While some criterion is not met,         (1) optimize the acquisition function to obtain the new best candidate,         (2) query the target function f,         (3) update the GP and the overall optimization state.     returns best found solution.\n\nArguments:\n\nBO::BOStruct: The Bayesian Optimization structure.\nstandardize: Specifies how to standardize the outputs.\nIf \"mean_scale\", standardize by removing mean and scaling by std.\nIf \"scale_only\", only scale the outputs without centering (in case we set a non-zero mean function with empirical mean).\nIf \"mean_only\", only remove the mean without scaling.\nIf nothing, do not standardize the outputs.\nhyper_params: Specifies how to handle hyperparameters.\nIf \"all\", re-optimize hyperparameters every 10 iterations.\nIf \"lengthscaleonly\", only optimize the lengthscale.\nIf nothing, do not re-optimize hyperparameters.\nnum_restarts_HP::Int: Number of random restarts for hyperparameter optimization.\n\nreturns:\n\nBO::BOStruct: The updated Bayesian Optimization problem after optimization.\nacqf_list::Vector: List of acquisition function values at each iteration.\nstandard_params::Tuple: Tuple containing the mean and standard deviation used for standardization\n\n\n\n\n\n","category":"function"},{"location":"reference/#Abstract-Interface","page":"Public API","title":"Abstract Interface","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.AbstractAcquisition","page":"Public API","title":"AbstractBayesOpt.AbstractAcquisition","text":"AbstractAcquisition\n\nAbstract type for acquisition functions used in Bayesian optimization.\n\nConcrete implementation should subtype this and implement the following methods:\n\n(acq::AbstractAcquisition)(surrogate::AbstractSurrogate, x::AbstractVector):   Evaluate the acquisition function at point x using the surrogate model.    This should also work for a single real input x::Real if working in 1D, in which case it is treated as a one-dimensional input vector. via the abstract method defined below.\nupdate(acq::AbstractAcquisition, ys::AbstractVector, model::AbstractSurrogate):   Update the acquisition function with new observations ys and the current surrogate model.\nBase.copy(acq::AbstractAcquisition):   Create a copy of the acquisition function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.AbstractDomain","page":"Public API","title":"AbstractBayesOpt.AbstractDomain","text":"AbstractDomain\n\nAn abstract type for defining the domain over which the optimization is performed.\n\nConcrete implementations should subtype this and define the necessary properties:\n\nlower: The lower bounds of the domain.\nupper: The upper bounds of the domain.\n\nas well as creating its constructor.\n\nOther methods can be added as needed depending on the use case.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.AbstractSurrogate","page":"Public API","title":"AbstractBayesOpt.AbstractSurrogate","text":"AbstractSurrogate\n\nAbstract type for surrogate models used in Bayesian optimization.\n\nConcrete implementation should subtype this and implement the following methods:\n\nupdate(model::AbstractSurrogate, xs::AbstractVector, ys::AbstractVector):   Update the surrogate model with new data points xs and corresponding observations ys.\nposterior_mean(surrogate::AbstractSurrogate, x::AbstractVector):   Compute the posterior mean of the surrogate model at point x.\nposterior_var(surrogate::AbstractSurrogate, x::AbstractVector):   Compute the posterior variance of the surrogate model at point x.\nnlml(surrogate::AbstractSurrogate, params::AbstractVector, xs::AbstractVector, ys::AbstractVector):   Compute the negative log marginal likelihood of the surrogate model given hyperparameters params, input data xs, and observations ys.\n\nIf you wish to standardize the outputs, you can also implement:\n\nstd_y(model::AbstractSurrogate):   Get the standard deviation used for standardizing the outputs in the surrogate model.\nget_mean_std(model::AbstractSurrogate):   Get the mean and standard deviation used for standardizing the outputs in the surrogate model.\n\nOther methods can be added as needed depending on the use case, and we refer to the impelementations of StandardGP and GradientGP for examples.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Surrogates","page":"Public API","title":"Surrogates","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.StandardGP","page":"Public API","title":"AbstractBayesOpt.StandardGP","text":"StandardGP{T}(gp::AbstractGPs.GP, noise_var::T, gpx::Union{Nothing,AbstractGPs.PosteriorGP}) <: AbstractSurrogate\n\nImplementation of the Abstract structures for the standard GP.\n\nAttributes:\n\ngp::AbstractGPs.GP: The underlying Gaussian Process model.\nnoise_var::T: The noise variance of the observations.\ngpx::Union{Nothing,AbstractGPs.PosteriorGP}: The posterior GP after conditioning on data, nothing if not conditioned yet.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.GradientGP","page":"Public API","title":"AbstractBayesOpt.GradientGP","text":"GradientGP{T}(gp::AbstractGps.GP, noise_var::T, p::Int, gpx::Union{Nothing,AbstractGPs.PosteriorGP}) <: AbstractSurrogate\n\nImplementation of the Abstract structures for the gradient-enhanced GP.\n\nThis relies on MOGP from AbstractGPs.jl and KernelFunctions.jl.\n\nAttributes:\n\ngp::AbstractGPs.GP: The underlying Gaussian Process model.\nnoise_var::T: The noise variance of the observations.\np::Int: The number of outputs (1 for function value + d for gradients).\ngpx::Union{Nothing,AbstractGPs.PosteriorGP}: The posterior GP after conditioning on data, nothing if not conditioned yet.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.posterior_mean","page":"Public API","title":"AbstractBayesOpt.posterior_mean","text":"posterior_mean(model::StandardGP, x::X) where {X}\n\nCompute the posterior mean of the GP at a new input point.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::X: A new input point where the prediction is to be made.\n\nreturns:\n\nmean: The posterior mean prediction at the input point.\n\n\n\n\n\nposterior_mean(model::StandardGP, x::AbstractVector)\n\nCompute the posterior mean of the GP at set of new input points.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::AbstractVector: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean: The posterior mean predictions at the input points.\n\n\n\n\n\nposterior_mean(model::GradientGP, x)\n\nCompute the function mean predictions of the GP model at new input points.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean::Vector: The mean predictions (function value only)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_var","page":"Public API","title":"AbstractBayesOpt.posterior_var","text":"posterior_var(model::StandardGP, x::X) where {X}\n\nCompute the posterior variance of the GP at a new input point.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::X: A new input point where the prediction is to be made.\n\nreturns:\n\nvar: The posterior variance prediction at the input point.\n\n\n\n\n\nposterior_var(model::StandardGP, x::AbstractVector)\n\nCompute the posterior variance of the GP at set of new input points.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::AbstractVector: A vector of new input points where predictions are to be made\n\nreturns:\n\nvar: The posterior variance predictions at the input points.\n\n\n\n\n\nposterior_var(model::GradientGP, x)\n\nCompute the function variance predictions of the GP model at new input points.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nvar::Vector: The variance predictions (function value only)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.nlml","page":"Public API","title":"AbstractBayesOpt.nlml","text":"nlml(model::StandardGP, params, xs::AbstractVector, ys::AbstractVector)\n\nCompute the negative log marginal likelihood (NLML) of the GP model given hyperparameters.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nparams: A vector containing the log lengthscale and log scale parameters.\nxs::AbstractVector: The input data points.\nys::AbstractVector: The observed function values.\n\nreturns:\n\nnlml::Float64: The negative log marginal likelihood of the model.\n\n\n\n\n\nnlml(model::GradientGP, params, xs::AbstractVector, ys::AbstractVector)\n\nCompute the negative log marginal likelihood (NLML) of the GP model given hyperparameters.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nparams: Parameters containing the log lengthscale and log scale.\nxs::AbstractVector: The input data points.\nys::AbstractVector: The observed function values and gradients.\n\nreturns:\n\nnlml::Float64: The negative log marginal likelihood of the model.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Kernels","page":"Public API","title":"Kernels","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.ApproxMatern52Kernel","page":"Public API","title":"AbstractBayesOpt.ApproxMatern52Kernel","text":"ApproxMatern52Kernel{M}(metric::M) <: KernelFunctions.SimpleKernel\n\nApproximate Matern 5/2 kernel using a second-order Taylor expansion around d=0.\n\nAttributes:\n\nmetric: The distance metric to be used, defaults to squared Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"reference/#GradientGP-related-functions","page":"Public API","title":"GradientGP-related functions","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.gradConstMean","page":"Public API","title":"AbstractBayesOpt.gradConstMean","text":"gradConstMean{V}(c::V)\n\nCustom mean function for the GradientGP model. Returns a constant per-output mean across MO inputs (function value + gradients). The first element corresponds to the function value, the following ones to the gradient outputs.\n\nUse gradConstMean([μ; zeros(d)]) to set a constant prior mean μ for the function value and zero for the gradients.\n\nAttributes:\n\nc::V: A vector of constants for each output (function value + gradients).\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.gradKernel","page":"Public API","title":"AbstractBayesOpt.gradKernel","text":"gradKernel{K}(base_kernel::K) <: MOKernel\n\nCustom kernel function for the GradientGP model that handles both function values and gradients.\n\nArguments:\n\nbase_kernel::KernelFunctions.Kernel: The base kernel function to be used.\n\nreturns:\n\ngradKernel: An instance of the custom gradient kernel function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.posterior_grad_mean","page":"Public API","title":"AbstractBayesOpt.posterior_grad_mean","text":"posterior_grad_mean(model::GradientGP, x)\n\nCompute the mean predictions of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean::Vector: The mean predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_grad_var","page":"Public API","title":"AbstractBayesOpt.posterior_grad_var","text":"posterior_grad_var(model::GradientGP, x)\n\nCompute the variance predictions of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nvar::Vector: The variance predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_grad_cov","page":"Public API","title":"AbstractBayesOpt.posterior_grad_cov","text":"posterior_grad_cov(model::GradientGP, x)\n\nCompute the covariance matrix of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\ncov::Matrix: The covariance matrix of the predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#Acquisition-Functions","page":"Public API","title":"Acquisition Functions","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.EnsembleAcquisition","page":"Public API","title":"AbstractBayesOpt.EnsembleAcquisition","text":"EnsembleAcquisition(weights::Vector{Float64}, acqs::Vector{AbstractAcquisition}) <: AbstractAcquisition\n\nAn ensemble acquisition function combines multiple acquisition functions, each weighted by a specified factor,\n\nAttributes:\n\nweights::Vector{Float64}: A vector of non-negative weights for each acquisition function. The weights are normalized to sum to 1.\nacquisitions::Vector{AbstractAcquisition}: A vector of acquisition functions to be combined.\n\nRemark: All weights must be non-negative.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ExpectedImprovement","page":"Public API","title":"AbstractBayesOpt.ExpectedImprovement","text":"ExpectedImprovement{Y}(ξ::Y, best_y::Y) <: AbstractAcquisition\n\nExpected Improvement acquisition function.\n\nAttributes:\n\nξ::Y: Exploration parameter\nbest_y::Y: Best observed objective value\n\nReferences: Jones et al., 1998\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.GradientNormUCB","page":"Public API","title":"AbstractBayesOpt.GradientNormUCB","text":"GradientNormUCB{Y}(β::Y) <: AbstractAcquisition\n\nAcquisition function implementing the Squared 2-norm of the gradient with Upper Confidence Bound (UCB) exploration strategy.\n\nAttributes:\n\nβ::Y: Exploration-exploitation balance parameter\n\nReferences:     Derived by Van Dieren, E. but open to previous references if existing.     Originally proposed by Makrygiorgos et al., 2023 but adapted to the squared 2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ProbabilityImprovement","page":"Public API","title":"AbstractBayesOpt.ProbabilityImprovement","text":"ProbabilityImprovement{Y}(ξ::Y, best_y::Y) <: AbstractAcquisition\n\nAttributes:\n\nξ::Y: Exploration parameter\nbest_y::Y: Best observed objective value\n\nReferences: Kushner, 1964\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.UpperConfidenceBound","page":"Public API","title":"AbstractBayesOpt.UpperConfidenceBound","text":"UpperConfidenceBound{Y}(β::Y) <: AbstractAcquisition\n\nUpper Confidence Bound (UCB) acquisition function.\n\nAttributes:\n\nβ::Y: Exploration-exploitation balance parameter\n\nReferences: Srinivas et al., 2012\n\n\n\n\n\n","category":"type"},{"location":"reference/#Domains","page":"Public API","title":"Domains","text":"","category":"section"},{"location":"reference/#Continuous-domain","page":"Public API","title":"Continuous domain","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.ContinuousDomain","page":"Public API","title":"AbstractBayesOpt.ContinuousDomain","text":"ContinuousDomain(lower::Vector{Float64}, upper::Vector{Float64}, bounds::Vector{Tuple{Float64,Float64}}) <: AbstractDomain\n\nA concrete implementation of AbstractDomain for continuous domains.\n\nAttributes:\n\nlower::Vector{Float64}: The lower bounds of the domain.\nupper::Vector{Float64}: The upper bounds of the domain.\nbounds::Vector{Tuple{Float64,Float64}}: A vector of tuples representing the (lower, upper) bounds for each dimension.\n\nConstructor:\n\nContinuousDomain(lower::Vector{Float64}, upper::Vector{Float64}):   Creates a ContinuousDomain instance given lower and upper bounds.   Performs sanity checks to ensure the bounds are valid.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractBayesOpt.jl","page":"Home","title":"AbstractBayesOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AbstractBayesOpt.jl is a Julia library for Bayesian Optimisation (BO), which relies on abstract classes for surrogate models, acquisition functions and domain definitions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The library is designed to solve minimisation problems of the form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"min_x in mathcalX f(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where f mathcalX to mathbbR is the objective function, which can be expensive to evaluate, non-differentiable, or noisy. The optimisation domain mathcalX subseteq mathbbR^d can be continuous, bounded, and possibly multi-dimensional.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The library uses Bayesian Optimisation (BO) to iteratively propose evaluation points x in the domain by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modeling the objective function with a surrogate model (e.g., Gaussian Process).\nUsing an acquisition function to select the next query point that balances exploration and exploitation.\nUpdating the surrogate with new observations and repeating until a stopping criterion is met.","category":"page"},{"location":"#How-AbstractBayesOpt.jl-fits-in-the-Julia-ecosystem","page":"Home","title":"How AbstractBayesOpt.jl fits in the Julia ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AbstractBayesOpt.jl provides a modular, abstract framework for Bayesian Optimisation in Julia. It defines three core abstractions: AbstractSurrogate, AbstractAcquisition, and AbstractDomain, as well as a standard optimisation loop, allowing users to plug in any surrogate model, acquisition function, or search space.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unlike traditional BO libraries that rely on a specific surrogate implementation (e.g. BayesianOptimization.jl using GaussianProcesses.jl), AbstractBayesOpt.jl is fully flexible. Users are free to use packages such as AbstractGPs.jl or GaussianProcesses.jl; in fact, our standard and gradient-enhanced GP implementations leverage AbstractGPs.jl and KernelFunctions.jl. We also mention the Surrogates.jl package, that implements a high level of BO using implemented surrogates (Kriging, Gradient-Enhanced Kriging, GPs from AbstractGPs.jl) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"In short, AbstractBayesOpt.jl acts as a general \"glue\" layer, unifying the Julia BO ecosystem under a simple and extensible interface.","category":"page"},{"location":"#Abstract-Interfaces","page":"Home","title":"Abstract Interfaces","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We currently have three main abstract interfaces that work with our BO loop:","category":"page"},{"location":"","page":"Home","title":"Home","text":"AbstractAcquisition: Interface to implement for an acquisition function to be used in AbstractBayesOpt.jl.\nAbstractDomain: Interface to implement for the optimisation domain to be used in AbstractBayesOpt.jl.\nAbstractSurrogate: Interface to implement for a surrogate to be used in AbstractBayesOpt.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AbstractBayesOpt.jl defines the core abstractions for building Bayesian optimisation algorithms. To add a new surrogate model, acquisition function, or domain, implement the following interfaces.","category":"page"},{"location":"#Acquisition-Functions","page":"Home","title":"Acquisition Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Subtype AbstractAcquisition and implement:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(acq::AbstractAcquisition)(model::AbstractSurrogate, x::AbstractVector):   Evaluate the acquisition function at x. We view x as a set of observations, and hence return a vector when we query acq.\nupdate(acq::AbstractAcquisition, ys::AbstractVector, model::AbstractSurrogate):   Update acquisition state given new observations.\nBase.copy(acq::AbstractAcquisition):   Return a copy of the acquisition function.","category":"page"},{"location":"#Domains","page":"Home","title":"Domains","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Subtype AbstractDomain and implement:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Concrete implementations should subtype this and define the necessary properties:","category":"page"},{"location":"","page":"Home","title":"Home","text":"lower: The lower bounds of the domain.\nupper: The upper bounds of the domain.","category":"page"},{"location":"","page":"Home","title":"Home","text":"as well as creating its constructor.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Concrete implementations may add additional methods as needed, but these are the minimum required for compatibility with the BO loop. We note that we are using Optim.jl to solve the  acquisition function maximisation problem for now,and hence the lower and  upper bounds must be compatible with their optimisation interface, which might  limit quite a lot the type of usable domains.","category":"page"},{"location":"#Surrogates","page":"Home","title":"Surrogates","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Subtype AbstractSurrogate and implement:","category":"page"},{"location":"","page":"Home","title":"Home","text":"update(model::AbstractSurrogate, xs::AbstractVector, ys::AbstractVector):   Update the surrogate with new data (xs, ys).\nposterior_mean(model::AbstractSurrogate, x):   Return the posterior mean at points x.\nposterior_var(model::AbstractSurrogate, x):   Return the posterior variance at points x.\nnlml(model::AbstractSurrogate, params, xs::AbstractVector, ys::AbstractVector):   Compute the negative log marginal likelihood given hyperparameters and data.","category":"page"},{"location":"#What-is-currently-implemented?","page":"Home","title":"What is currently implemented?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We list below the abstract subtypes currently implemented in AbstractBayesOpt.jl.","category":"page"},{"location":"#Surrogates-2","page":"Home","title":"Surrogates","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StandardGP: Gaussian Process surrogate model with standard mean and covariance functions.\nGradientGP: Gaussian Process surrogate model supporting gradient information.","category":"page"},{"location":"#Acquisition-functions","page":"Home","title":"Acquisition functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ExpectedImprovement: Standard expected improvement acquisition function for balancing exploration and exploitation.\nUpperConfidenceBound: Acquisition function using a confidence bound to guide optimisation.\nGradientNormUCB: Gradient-based variant of the Upper Confidence Bound acquisition function.\nProbabilityImprovement: Probability of improvement acquisition function.\nEnsembleAcquisition: Combines multiple acquisition functions into an ensemble to leverage complementary strategies.","category":"page"},{"location":"#Domains-2","page":"Home","title":"Domains","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ContinuousDomain: Represents a continuous optimisation domain, defining bounds and dimensionality for optimisation problems.","category":"page"},{"location":"tutorials/1D_BO/#AbstractBayesOpt-Tutorial:-1D-Bayesian-Optimisation","page":"1D Bayesian Optimisation","title":"AbstractBayesOpt Tutorial: 1D Bayesian Optimisation","text":"","category":"section"},{"location":"tutorials/1D_BO/#Setup","page":"1D Bayesian Optimisation","title":"Setup","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"Loading the necessary packages.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"using AbstractBayesOpt\nusing AbstractGPs\nusing ForwardDiff\nusing Plots\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nusing Random # hide\nRandom.seed!(42) # hide\nnothing # hide","category":"page"},{"location":"tutorials/1D_BO/#Define-the-objective-function","page":"1D Bayesian Optimisation","title":"Define the objective function","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"We will optimise a simple 1D function: f(x) = (x-2)^2 + sin(3x)","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"f(x) = (x - 2)^2 + sin(3x)\nmin_f = -0.8494048256167165 # hide\nd = 1\ndomain = ContinuousDomain([0.0], [5.0])\n\nplot_domain = domain.lower[1]:0.01:domain.upper[1] #hide\nys = f.(plot_domain) #hide\n\nplot(                                           #hide\n    plot_domain,                                #hide\n    ys;                                          #hide\n    xlim=(domain.lower[1], domain.upper[1]),    #hide\n    label=\"f(x)\",               #hide\n    xlabel=\"x\",                 #hide\n    ylabel=\"f(x)\",             #hide\n    legend=:outertopright,              #hide\n) #hide\n\nx_min = plot_domain[argmin(ys)] #hide\nscatter!([x_min], [minimum(ys)]; label=\"Minimum\", color=:red, markersize=5) #hide","category":"page"},{"location":"tutorials/1D_BO/#Standard-GPs","page":"1D Bayesian Optimisation","title":"Standard GPs","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"We'll use a standard Gaussian Process surrogate with a Matérn 5/2 kernel. We add a small jitter term for numerical stability of 10^-12.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"noise_var = 1e-12\nsurrogate = StandardGP(Matern52Kernel(), noise_var)","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"Generate uniform random samples x_train and evaluate the function at these points to get y_train.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"n_train = 5\nx_train = first.([\n    domain.lower .+ (domain.upper .- domain.lower) .* rand(d) for _ in 1:n_train\n])\n\ny_train = f.(x_train)","category":"page"},{"location":"tutorials/1D_BO/#Choose-an-acquisition-function","page":"1D Bayesian Optimisation","title":"Choose an acquisition function","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"We'll use the Expected Improvement acquisition function with an exploration parameter ξ = 0.0.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"ξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))","category":"page"},{"location":"tutorials/1D_BO/#Set-up-the-Bayesian-Optimisation-structure","page":"1D Bayesian Optimisation","title":"Set up the Bayesian Optimisation structure","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"We use BOStruct to bundle all components needed for the optimisation. Here, we set the number of iterations to 5 and the actual noise level to 0.0 (since our function is noiseless). We then run the optimize function to perform the Bayesian optimisation.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"bo_struct = BOStruct(\n    f,\n    acq,\n    surrogate,\n    domain,\n    x_train,\n    y_train,\n    30,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian ...\"\nresult, acq_list, standard_params = AbstractBayesOpt.optimize(\n    bo_struct; standardize=\"mean_only\"\n);\nnothing #hide","category":"page"},{"location":"tutorials/1D_BO/#Results","page":"1D Bayesian Optimisation","title":"Results","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"The  result is stored in result. We can print the best found input and its corresponding function value.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"xs = result.xs # hide\nys = result.ys_non_std # hide\n\nprintln(\"Optimal point: \", xs[argmin(ys)]) # hide\nprintln(\"Optimal value: \", minimum(ys)) # hide","category":"page"},{"location":"tutorials/1D_BO/#Plotting-of-running-minimum-over-iterations","page":"1D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"The running minimum is the best function value found up to each iteration.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"running_min = accumulate(min, f.(xs)) # hide\n\np = Plots.plot( # hide\n    n_train:length(running_min),  # hide\n    running_min[n_train:end] .- min_f;  # hide\n    yaxis=:log,  # hide\n    title=\"Error w.r.t true minimum (1D BO)\",  # hide\n    xlabel=\"Function evaluations\",  # hide\n    label=\"BO\",  # hide\n    xlims=(1, length(running_min)),  # hide\n) # hide\nPlots.vspan!([1, n_train]; color=:blue, alpha=0.2, label=\"training GP\") # hide","category":"page"},{"location":"tutorials/1D_BO/#Gradient-enhanced-GPs","page":"1D Bayesian Optimisation","title":"Gradient-enhanced GPs","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"Now, let's see how to use gradient information to improve the optimisation. We'll use the same function but now also provide its gradient. We define a new surrogate model that can handle gradient information, specifically a GradientGP.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"grad_surrogate = GradientGP(ApproxMatern52Kernel(), d + 1, noise_var)\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))\n\n∂f(x) = ForwardDiff.derivative(f, x)\nf_∂f(x) = [f(x); ∂f(x)];\nnothing #hide","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"Generate value and gradients at random samples","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"y_train_grad = f_∂f.(x_train)","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"Set up the Bayesian Optimisation structure","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"bo_struct_grad = BOStruct(\n    f_∂f,\n    acq,\n    grad_surrogate,\n    domain,\n    x_train,\n    y_train_grad,\n    10,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\" # hide\nresult_grad, acq_list_grad, standard_params_grad = AbstractBayesOpt.optimize(\n    bo_struct_grad; standardize=\"mean_only\"\n);\nnothing #hide","category":"page"},{"location":"tutorials/1D_BO/#Results-2","page":"1D Bayesian Optimisation","title":"Results","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"The  result is stored in result_grad. We can print the best found input and its corresponding function value.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"xs_grad = reduce(vcat, result_grad.xs) # hide\nys_grad = hcat(result_grad.ys_non_std...)[1, :] # hide\n\nprintln(\"Optimal point (GradBO): \", xs_grad[argmin(ys_grad)]) # hide\nprintln(\"Optimal value (GradBO): \", minimum(ys_grad)) # hide","category":"page"},{"location":"tutorials/1D_BO/#Plotting-of-running-minimum-over-iterations-2","page":"1D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"The running minimum is the best function value found up to each iteration. Since each evaluation provides both a function value and a 1D gradient, we duplicate the running minimum values to reflect the number of function evaluations.","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"running_min_grad = accumulate(min, f.(xs_grad)) # hide\nrunning_min_grad = collect(Iterators.flatten(fill(x, 2) for x in (running_min_grad))) # hide\n\np = Plots.plot( # hide\n    (2 * n_train):length(running_min_grad), # hide\n    running_min_grad[(2 * n_train):end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (1D GradBO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"gradBO\", # hide\n    xlims=(1, length(running_min_grad)), # hide\n) # hide\nPlots.vspan!([1, 2 * n_train]; color=:blue, alpha=0.2, label=\"\") # hide","category":"page"},{"location":"tutorials/1D_BO/#Plotting-the-surrogate-model","page":"1D Bayesian Optimisation","title":"Plotting the surrogate model","text":"","category":"section"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"We can visualize the surrogate model's mean and uncertainty along with the true function and the evaluated","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"plot_domain = collect(domain.lower[1]:0.01:domain.upper[1])\n\nplot_x = map(x -> [x], plot_domain)\nplot_x = prep_input(grad_surrogate, plot_x)\npost_mean, post_var = unstandardized_mean_and_var(\n    result_grad.model, plot_x, standard_params_grad\n)\n\npost_mean = reshape(post_mean, :, d + 1)[:, 1]\npost_var = reshape(post_var, :, d + 1)[:, 1]\npost_var[post_var .< 0] .= 0\n\nplot( #hide\n    plot_domain, # hide\n    f.(plot_domain); # hide\n    label=\"target function\", # hide\n    xlim=(domain.lower[1], domain.upper[1]), # hide\n    xlabel=\"x\", # hide\n    ylabel=\"y\", # hide\n    title=\"AbstractBayesOpt\", # hide\n    legend=:outertopright, # hide\n) # hide\nplot!( #hide\n    plot_domain, # hide\n    post_mean; # hide\n    label=\"gradGP\", # hide\n    ribbon=sqrt.(post_var), # hide\n    ribbon_scale=2, # hide\n    color=\"green\", # hide\n) # hide\nscatter!(xs_grad[1:n_train], ys_grad[1:n_train]; label=\"Train Data\") # hide\nscatter!(xs_grad[(n_train + 1):end], ys_grad[(n_train + 1):end]; label=\"Candidates\") # hide\nscatter!([xs_grad[argmin(ys_grad)]], [minimum(ys_grad)]; label=\"Best candidate\") # hide","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"","category":"page"},{"location":"tutorials/1D_BO/","page":"1D Bayesian Optimisation","title":"1D Bayesian Optimisation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/hyperparams_comparison/#AbstractBayesOpt-Tutorial:-Hyperparameter-Tuning-and-Standardisation-Comparison","page":"Hyperparameter & Standardisation Comparison","title":"AbstractBayesOpt Tutorial: Hyperparameter Tuning and Standardisation Comparison","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"This tutorial presents the different options for hyperparameter optimisation and standardisation modes available in AbstractBayesOpt.jl. We will compare the performance of these configurations on gradient-enhanced GPs on the Himmelblau function","category":"page"},{"location":"tutorials/hyperparams_comparison/#Setup","page":"Hyperparameter & Standardisation Comparison","title":"Setup","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"Loading the necessary packages.","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"using AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(42) # hide\nnothing # hide","category":"page"},{"location":"tutorials/hyperparams_comparison/#Define-the-objective-function","page":"Hyperparameter & Standardisation Comparison","title":"Define the objective function","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"We will use the Himmelblau function, a well-known multi-modal test function with four global minima. The function is defined as: f(x_1 x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"himmelblau(x::AbstractVector) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n∇himmelblau(x) = ForwardDiff.gradient(himmelblau, x)","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"Combined function that returns both value and gradient for our gradient-enhanced GP","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"f_val_grad(x) = [himmelblau(x); ∇himmelblau(x)]\n\nglobal_min = 0.0 # hide\n\nd = 2\nlower = [-6.0, -6.0]\nupper = [6.0, 6.0]\ndomain = ContinuousDomain(lower, upper)\n\nresolution = 100 # hide\nX = range(lower[1], upper[1]; length=resolution) # hide\nY = range(lower[2], upper[2]; length=resolution) # hide\nx_mins = [[3.0, 2.0], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]] # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> himmelblau([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Himmelblau Function\", # hide\n    xlabel=\"x₁\", # hide\n    ylabel=\"x₂\", # hide\n) # hide\n\nscatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"Global minima\", # hide\n    color=:red, # hide\n    markersize=5, # hide\n    legend=:bottomright, # hide\n) # hide","category":"page"},{"location":"tutorials/hyperparams_comparison/#Initial-Training-Data-and-Model-Setup","page":"Hyperparameter & Standardisation Comparison","title":"Initial Training Data and Model Setup","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"We'll use a gradient-enhanced Gaussian Process with an approximate Matérn 5/2 kernel. For better space coverage, we generate initial training data using Sobol sampling.","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"σ² = 1e-12\n\nn_train = 8\nx_train = [\n    collect(col) for\n    col in eachcol(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\n]","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"Evaluate function and gradients at training points","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"y_train = f_val_grad.(x_train)","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"Setup the base model that we'll use across all configurations","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"base_model = GradientGP(ApproxMatern52Kernel(), d+1, σ²)","category":"page"},{"location":"tutorials/hyperparams_comparison/#Configuration-Setup","page":"Hyperparameter & Standardisation Comparison","title":"Configuration Setup","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"We'll compare different combinations of hyperparameter optimisation strategies and standardisation modes. This comprehensive comparison will help us understand how these settings affect optimisation performance.","category":"page"},{"location":"tutorials/hyperparams_comparison/#Hyperparameter-Optimisation-Strategies:","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter Optimisation Strategies:","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"\"all\": Optimise all kernel hyperparameters (length scales, signal variance, etc.)\n\"lengthscaleonly\": Only optimise the length scale parameters\nnothing: Use fixed hyperparameters (no optimisation)","category":"page"},{"location":"tutorials/hyperparams_comparison/#Standardisation-Modes:","page":"Hyperparameter & Standardisation Comparison","title":"Standardisation Modes:","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"\"mean_scale\": Remove empirical mean and scale by standard deviation (default)\n\"scale_only\": Only scale by standard deviation\n\"mean_only\": Only remove empirical mean, no scaling\nnothing: No standardisation applied","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"test_configs = [ # hide\n    (\"HP:all + MeanScale\", \"all\", \"mean_scale\"), # hide\n    (\"HP:all + ScaleOnly\", \"all\", \"scale_only\"), # hide\n    (\"HP:all + MeanOnly\", \"all\", \"mean_only\"), # hide\n    (\"HP:all + NoStd\", \"all\", nothing), # hide\n    (\"HP:length + MeanScale\", \"length_scale_only\", \"mean_scale\"), # hide\n    (\"HP:length + ScaleOnly\", \"length_scale_only\", \"scale_only\"), # hide\n    (\"HP:length + MeanOnly\", \"length_scale_only\", \"mean_only\"), # hide\n    (\"HP:length + NoStd\", \"length_scale_only\", nothing), # hide\n    (\"HP:none + MeanScale\", nothing, \"mean_scale\"), # hide\n    (\"HP:none + ScaleOnly\", nothing, \"scale_only\"), # hide\n    (\"HP:none + MeanOnly\", nothing, \"mean_only\"), # hide\n    (\"HP:none + NoStd\", nothing, nothing), # hide\n]# hide","category":"page"},{"location":"tutorials/hyperparams_comparison/#Running-the-Optimisation-Comparison","page":"Hyperparameter & Standardisation Comparison","title":"Running the Optimisation Comparison","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"We will run Bayesian optimisation with each configuration and collect performance metrics.","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"function run_comparison(n_iterations) # hide\n    results = Dict{String,NamedTuple}() # hide\n\n    for (config_name, hyper_params, standardise_mode) in test_configs # hide\n        model = deepcopy(base_model) # hide\n\n        best_y = minimum(first.(y_train)) # hide\n        acq_func = ExpectedImprovement(0.0, best_y) # hide\n\n        problem = BOStruct( # hide\n            f_val_grad, # hide\n            acq_func, # hide\n            model, # hide\n            domain, # hide\n            x_train, # hide\n            y_train, # hide\n            n_iterations, # hide\n            0.0,   # hide\n        ) # hide\n\n        start_time = time() # hide\n\n        try # hide\n            result, _, standard_params = AbstractBayesOpt.optimize( # hide\n                problem;\n                hyper_params=hyper_params,\n                standardize=standardise_mode, # hide\n            ) # hide\n\n            end_time = time() # hide\n            elapsed_time = end_time - start_time # hide\n\n            xs = result.xs # hide\n            ys_values = first.(result.ys_non_std) # hide\n\n            optimal_idx = argmin(ys_values) # hide\n            optimal_point = xs[optimal_idx] # hide\n            optimal_value = minimum(ys_values) # hide\n\n            all_evals = himmelblau.(xs) # hide\n            running_min = accumulate(min, all_evals) # hide\n\n            errors = max.(running_min .- global_min, 1e-16) # hide\n\n            results[config_name] = ( # hide\n                xs=xs, # hide\n                ys_values=ys_values, # hide\n                running_min=running_min, # hide\n                errors=errors, # hide\n                optimal_point=optimal_point, # hide\n                optimal_value=optimal_value, # hide\n                error_from_global=abs(optimal_value - global_min), # hide\n                elapsed_time=elapsed_time, # hide\n                hyper_params=hyper_params, # hide\n                standardize=standardise_mode, # hide\n                standard_params=standard_params, # hide\n                n_evaluations=length(xs), # hide\n            ) # hide\n\n        catch e # hide\n            @warn \"ERROR in configuration $config_name: $e\" # hide\n            results[config_name] = ( # hide\n                error_from_global=Inf, # hide\n                elapsed_time=Inf, # hide\n                hyper_params=hyper_params, # hide\n                standardize=standardise_mode, # hide\n                n_evaluations=0, # hide\n            ) # hide\n        end # hide\n    end # hide\n\n    return results # hide\nend; # hide\nnothing #hide","category":"page"},{"location":"tutorials/hyperparams_comparison/#Execute-the-comparison","page":"Hyperparameter & Standardisation Comparison","title":"Execute the comparison","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"Let's run the optimisation with all 12 different configurations. This will take some time as we're testing various combinations of hyperparameter optimisation and standardisation settings.","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"@info \"Starting comparison...\" # hide\nresults = run_comparison(30)","category":"page"},{"location":"tutorials/hyperparams_comparison/#Results-Analysis-and-Visualisation","page":"Hyperparameter & Standardisation Comparison","title":"Results Analysis and Visualisation","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"function plot_convergence_comparison(results) # hide\n    p = plot(; # hide\n        title=\"Himmelblau Optimisation: Hyperparameter & Standardisation Comparison\", # hide\n        xlabel=\"Number of iterations\", # hide\n        ylabel=\"Error from global minimum\", # hide\n        yaxis=:log, # hide\n        legend=:bottomleft, # hide\n        linewidth=2, # hide\n        size=(1200, 800), # hide\n    ) # hide\n\n    colors = [ # hide\n        :blue, # hide\n        :lightblue, # hide\n        :cyan, # hide\n        :gray, # hide\n        :red, # hide\n        :pink, # hide\n        :orange, # hide\n        :brown, # hide\n        :green, # hide\n        :lightgreen, # hide\n        :yellow, # hide\n        :purple, # hide\n    ] # hide\n    styles = [ # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n    ] # hide\n\n    config_names = [ # hide\n        \"HP:all + MeanScale\", # hide\n        \"HP:all + ScaleOnly\", # hide\n        \"HP:all + MeanOnly\", # hide\n        \"HP:all + NoStd\", # hide\n        \"HP:length + MeanScale\", # hide\n        \"HP:length + ScaleOnly\", # hide\n        \"HP:length + MeanOnly\", # hide\n        \"HP:length + NoStd\", # hide\n        \"HP:none + MeanScale\", # hide\n        \"HP:none + ScaleOnly\", # hide\n        \"HP:none + MeanOnly\", # hide\n        \"HP:none + NoStd\", # hide\n    ] # hide\n\n    for (i, config_name) in enumerate(config_names) # hide\n        if haskey(results, config_name) && haskey(results[config_name], :errors) # hide\n            result = results[config_name] # hide\n            plot!( # hide\n                p, # hide\n                1:length(result.errors), # hide\n                result.errors; # hide\n                label=config_name, # hide\n                color=colors[i], # hide\n                linestyle=styles[i], # hide\n            ) # hide\n        end # hide\n    end # hide\n\n    vspan!(p, [1, n_train]; color=:gray, alpha=0.2, label=\"Initial data\") # hide\n\n    return p # hide\nend; #hide\nnothing #hide","category":"page"},{"location":"tutorials/hyperparams_comparison/#Create-and-display-the-convergence-plot","page":"Hyperparameter & Standardisation Comparison","title":"Create and display the convergence plot","text":"","category":"section"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"This comprehensive plot shows the optimisation performance of all 12 configurations. Each line represents a different combination of hyperparameter optimisation and standardisation.","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"conv_plot = plot_convergence_comparison(results) # hide","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"","category":"page"},{"location":"tutorials/hyperparams_comparison/","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter & Standardisation Comparison","text":"This page was generated using Literate.jl.","category":"page"}]
}
