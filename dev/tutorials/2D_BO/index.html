<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>2D Bayesian Optimisation · AbstractBayesOpt.jl</title><meta name="title" content="2D Bayesian Optimisation · AbstractBayesOpt.jl"/><meta property="og:title" content="2D Bayesian Optimisation · AbstractBayesOpt.jl"/><meta property="twitter:title" content="2D Bayesian Optimisation · AbstractBayesOpt.jl"/><meta name="description" content="Documentation for AbstractBayesOpt.jl."/><meta property="og:description" content="Documentation for AbstractBayesOpt.jl."/><meta property="twitter:description" content="Documentation for AbstractBayesOpt.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AbstractBayesOpt.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../reference/">Public API</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../1D_BO/">1D Bayesian Optimisation</a></li><li class="is-active"><a class="tocitem" href>2D Bayesian Optimisation</a><ul class="internal"><li><a class="tocitem" href="#Setup"><span>Setup</span></a></li><li><a class="tocitem" href="#Define-the-objective-function"><span>Define the objective function</span></a></li><li><a class="tocitem" href="#Standard-GPs"><span>Standard GPs</span></a></li><li><a class="tocitem" href="#Gradient-enhanced-GPs"><span>Gradient-enhanced GPs</span></a></li></ul></li><li><a class="tocitem" href="../acq_funcs_comparison/">1D Acquisition Function Comparison</a></li><li><a class="tocitem" href="../hyperparams_comparison/">Hyperparameter &amp; Standardisation Comparison</a></li><li><a class="tocitem" href="../nlml_landscape_visualization/">Log Likelihood Landscape Visualisation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>2D Bayesian Optimisation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>2D Bayesian Optimisation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/evandieren/AbstractBayesOpt.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/evandieren/AbstractBayesOpt.jl/blob/main/docs/literate/tutorials/2D_BO.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation"><a class="docs-heading-anchor" href="#AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation">AbstractBayesOpt Tutorial: Basic 2D Optimisation</a><a id="AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation-1"></a><a class="docs-heading-anchor-permalink" href="#AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation" title="Permalink"></a></h1><h2 id="Setup"><a class="docs-heading-anchor" href="#Setup">Setup</a><a id="Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Setup" title="Permalink"></a></h2><p>Loading the necessary packages.</p><pre><code class="language-julia hljs">using AbstractBayesOpt
using AbstractGPs
using ForwardDiff
using Plots</code></pre><h2 id="Define-the-objective-function"><a class="docs-heading-anchor" href="#Define-the-objective-function">Define the objective function</a><a id="Define-the-objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-objective-function" title="Permalink"></a></h2><pre><code class="language-julia hljs">f(x) = (x[1]^2 + x[2] - 11)^2 + (x[1]+x[2]^2-7)^2
d = 2
domain = ContinuousDomain([-6.0, -6.0], [6.0, 6.0])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Vector{Float64}}:
 [3.0, 2.0]
 [-2.805118, 3.131312]
 [-3.77931, -3.283186]
 [3.584428, -1.848126]</code></pre><p>Scatter them on the contour plot</p><img src="a56c4582.svg" alt="Example block output"/><h2 id="Standard-GPs"><a class="docs-heading-anchor" href="#Standard-GPs">Standard GPs</a><a id="Standard-GPs-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-GPs" title="Permalink"></a></h2><p>We&#39;ll use a standard Gaussian Process surrogate with a squared-exponential kernel. We add a small jitter term for numerical stability of <span>$10^{-9}$</span>.</p><pre><code class="language-julia hljs">noise_var = 1e-9
surrogate = StandardGP(SqExponentialKernel(), noise_var)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StandardGP{Float64}(AbstractGPs.GP{AbstractGPs.ZeroMean{Float64}, KernelFunctions.ScaledKernel{KernelFunctions.TransformedKernel{KernelFunctions.SqExponentialKernel{Distances.Euclidean}, KernelFunctions.ScaleTransform{Float64}}, Float64}}(AbstractGPs.ZeroMean{Float64}(), Squared Exponential Kernel (metric = Distances.Euclidean(0.0))
	- Scale Transform (s = 1.0)
	- σ² = 1.0), 1.0e-9, nothing)</code></pre><p>Generate uniform random samples x_train</p><pre><code class="language-julia hljs">n_train = 5
x_train = [domain.lower .+ (domain.upper .- domain.lower) .* rand(d) for _ in 1:n_train]

y_train = f.(x_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
 110.33292690679502
  73.82945209082831
 238.53003395234026
  53.19981955218784
 175.66046578818757</code></pre><h3 id="Choose-an-acquisition-function"><a class="docs-heading-anchor" href="#Choose-an-acquisition-function">Choose an acquisition function</a><a id="Choose-an-acquisition-function-1"></a><a class="docs-heading-anchor-permalink" href="#Choose-an-acquisition-function" title="Permalink"></a></h3><p>We&#39;ll use the Expected Improvement acquisition function with an exploration parameter ξ = 0.0.</p><pre><code class="language-julia hljs">ξ = 0.0
acq = ExpectedImprovement(ξ, minimum(y_train))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ExpectedImprovement{Float64}(0.0, 53.19981955218784)</code></pre><h3 id="Set-up-the-Bayesian-Optimisation-structure"><a class="docs-heading-anchor" href="#Set-up-the-Bayesian-Optimisation-structure">Set up the Bayesian Optimisation structure</a><a id="Set-up-the-Bayesian-Optimisation-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Set-up-the-Bayesian-Optimisation-structure" title="Permalink"></a></h3><p>We use BOStruct to bundle all components needed for the optimization. Here, we set the number of iterations to 5 and the actual noise level to 0.0 (since our function is noiseless). We then run the optimize function to perform the Bayesian Optimisation.</p><pre><code class="language-julia hljs">bo_struct = BOStruct(
    f,
    acq,
    surrogate,
    domain,
    x_train,
    y_train,
    50,  # number of iterations
    0.0,  # Actual noise level (0.0 for noiseless)
)

@info &quot;Starting Bayesian Optimisation...&quot;
result, acq_list, standard_params = AbstractBayesOpt.optimize(
    bo_struct; standardize=&quot;mean_only&quot;
);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Starting Bayesian Optimisation...
[ Info: Standardization choice: mean_only
[ Info: Standardization parameters: μ=130.3105396580678, σ=1.0
[ Info: Optimizing GP hyperparameters at iteration 1...
[ Info: New parameters: ℓ=[4.3301786775537305], variance =[7070.708258943665]
[ Info: Iteration #1, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [5.115857383078854, 3.2143239896890967]
[ Info: Iteration #2, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [0.845424229971077, 1.9173484254543451]
[ Info: Iteration #3, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [-1.3377578693672527, 5.999999999999999]
[ Info: Iteration #4, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [-5.999999999999981, 0.7512300985628113]
[ Info: Iteration #5, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [4.613860395345961, -0.9168867080225622]
[ Info: Iteration #6, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [3.3356383514066703, -0.06409933386184638]
[ Info: Iteration #7, current min val: 13.401343215764077
[ Info: Acquisition optimized, new candidate point: [-3.0374521124694067, -5.999999999999999]
[ Info: Iteration #8, current min val: 13.401343215764077
[ Info: Acquisition optimized, new candidate point: [3.3351826864795333, 0.5905594985460638]
[ Info: Iteration #9, current min val: 11.506033005543554
[ Info: Acquisition optimized, new candidate point: [3.358281988610822, 0.42165060288073747]
[ Info: Iteration #10, current min val: 11.506033005543554
[ Info: Acquisition optimized, new candidate point: [2.874086567664644, 1.8059265484204396]
[ Info: Optimizing GP hyperparameters at iteration 11...
[ Info: New parameters: ℓ=[2.8887734130214517], variance =[197318.28899774252]
[ Info: Iteration #11, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [5.999999999999978, -5.999999999958906]
[ Info: Iteration #12, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-5.999999999985845, 5.999999999999548]
[ Info: Iteration #13, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-2.2623215742391523, 0.6775338380036582]
[ Info: Iteration #14, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-5.999999999999999, -3.768509006238866]
[ Info: Iteration #15, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [2.821100758710569, 5.999999999999999]
[ Info: Iteration #16, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [2.246495861203007, -2.0873713583090647]
[ Info: Iteration #17, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-2.094950424532763, 2.2718433079116247]
[ Info: Iteration #18, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [2.773973839557414, 1.557132308351617]
[ Info: Iteration #19, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [0.3536915882896765, -5.999999999999999]
[ Info: Iteration #20, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-5.999999999990384, -5.999999999999963]
[ Info: Optimizing GP hyperparameters at iteration 21...
[ Info: New parameters: ℓ=[3.417415218971281], variance =[999999.9999999978]
[ Info: Iteration #21, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-3.358888978360358, -2.9264761713569896]
[ Info: Iteration #22, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [3.43582441101804, -2.2369748452602476]
[ Info: Iteration #23, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-2.8043175285364335, -2.2528449614331416]
[ Info: Iteration #24, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [-3.207210645331833, -3.4127156797771434]
[ Info: Iteration #25, current min val: 1.6192295547619184
[ Info: Acquisition optimized, new candidate point: [3.4668063152617004, -1.7055652578634786]
[ Info: Iteration #26, current min val: 0.8613972763586741
[ Info: Acquisition optimized, new candidate point: [-3.7743126711432766, -3.4069104522919713]
[ Info: Iteration #27, current min val: 0.7195068092127793
[ Info: Acquisition optimized, new candidate point: [-3.6878809266606725, -3.325983611947392]
[ Info: Iteration #28, current min val: 0.6664662530676188
[ Info: Acquisition optimized, new candidate point: [3.0184367900409024, 1.9007217285891957]
[ Info: Iteration #29, current min val: 0.136164759329886
[ Info: Acquisition optimized, new candidate point: [-3.360533062387275, 1.9092921904005749]
[ Info: Iteration #30, current min val: 0.136164759329886
[ Info: Acquisition optimized, new candidate point: [2.997132828884206, 1.9981121548408816]
[ Info: Optimizing GP hyperparameters at iteration 31...
[ Info: New parameters: ℓ=[3.359600025724728], variance =[999999.9999999978]
[ Info: Iteration #31, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.4923385766355928, 3.279148009296276]
[ Info: Iteration #32, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.7704871867183667, 3.0116868268533734]
[ Info: Iteration #33, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.6206009296379076, 3.006889756234223]
[ Info: Iteration #34, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.8018356021271953, 3.1603783149251554]
[ Info: Iteration #35, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [3.620148121301583, -1.8682904153531283]
[ Info: Iteration #36, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [3.57815232490016, -1.8436775411872912]
[ Info: Iteration #37, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.8171312471351526, 3.1272362823807653]
[ Info: Iteration #38, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [3.58215563611747, -1.8715845434277032]
[ Info: Iteration #39, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-3.7903616931423527, -3.283059260236909]
[ Info: Iteration #40, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-3.7856114892200954, -3.2968160932744324]
[ Info: Optimizing GP hyperparameters at iteration 41...
[ Info: New parameters: ℓ=[3.220359159229683], variance =[999999.9999999978]
[ Info: Iteration #41, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [5.9723067897678375, 5.999999999999999]
[ Info: Iteration #42, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-2.801276821367239, 3.1302710598448553]
[ Info: Iteration #43, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [3.7098000000000013, -5.9526]
[ Info: Iteration #44, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [5.959800000000001, -3.3114]
[ Info: Iteration #45, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-5.9886, 3.2958]
[ Info: Iteration #46, current min val: 0.0004726195469245476
[ Info: Acquisition optimized, new candidate point: [-3.77827863665996, -3.2823237591587366]
[ Info: Iteration #47, current min val: 6.951568578864588e-5
[ Info: Acquisition optimized, new candidate point: [0.9222000000000001, 4.212600000000002]
[ Info: Iteration #48, current min val: 6.951568578864588e-5
[ Info: Acquisition optimized, new candidate point: [-3.6605999999999996, 5.9886]
[ Info: Iteration #49, current min val: 6.951568578864588e-5
[ Info: Acquisition optimized, new candidate point: [5.9862, 1.8714000000000004]
[ Info: Iteration #50, current min val: 6.951568578864588e-5
[ Info: Acquisition optimized, new candidate point: [-4.0314, -0.9233999999999991]</code></pre><h3 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h3><p>The optimization result is stored in <code>result</code>. We can print the best found input and its corresponding function value.</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimal point: [-3.77827863665996, -3.2823237591587366]
Optimal value: 6.951568578864588e-5</code></pre><h3 id="Plotting-of-running-minimum-over-iterations"><a class="docs-heading-anchor" href="#Plotting-of-running-minimum-over-iterations">Plotting of running minimum over iterations</a><a id="Plotting-of-running-minimum-over-iterations-1"></a><a class="docs-heading-anchor-permalink" href="#Plotting-of-running-minimum-over-iterations" title="Permalink"></a></h3><p>The running minimum is the best function value found up to each iteration.</p><img src="eaadbe5c.svg" alt="Example block output"/><h2 id="Gradient-enhanced-GPs"><a class="docs-heading-anchor" href="#Gradient-enhanced-GPs">Gradient-enhanced GPs</a><a id="Gradient-enhanced-GPs-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-enhanced-GPs" title="Permalink"></a></h2><p>Now, let&#39;s see how to use gradient information to improve the optimization. We&#39;ll use the same function but now also provide its gradient. We define a new surrogate model that can handle gradient information, specifically a <code>GradientGP</code>.</p><pre><code class="language-julia hljs">grad_surrogate = GradientGP(SqExponentialKernel(), d + 1, noise_var)

ξ = 0.0
acq = ExpectedImprovement(ξ, minimum(y_train))

∇f(x) = ForwardDiff.gradient(f, x)
f_val_grad(x) = [f(x); ∇f(x)];</code></pre><p>Generate value and gradients at random samples</p><pre><code class="language-julia hljs">y_train_grad = f_val_grad.(x_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Vector{Float64}}:
 [110.33292690679502, -67.222233853589, -6.233898817174085]
 [73.82945209082831, 6.547041429739899, -29.940074643607936]
 [238.53003395234026, -66.57395175082759, -200.2478970556137]
 [53.19981955218784, -41.927777146518736, -26.836469804291816]
 [175.66046578818757, 23.666930366717853, -9.728624459781422]</code></pre><p>Set up the Bayesian Optimisation structure</p><pre><code class="language-julia hljs">bo_struct_grad = BOStruct(
    f_val_grad,
    acq,
    grad_surrogate,
    domain,
    x_train,
    y_train_grad,
    20,  # number of iterations
    0.0,  # Actual noise level (0.0 for noiseless)
)

result_grad, acq_list_grad, standard_params_grad = AbstractBayesOpt.optimize(bo_struct_grad);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Starting Bayesian Optimisation...
[ Info: Standardization choice: mean_scale
[ Info: Standardization parameters: μ=[130.3105396580678, 0.0, 0.0], σ=[76.32718415574614, 76.32718415574614, 76.32718415574614]
[ Info: Optimizing GP hyperparameters at iteration 1...
[ Info: New parameters: ℓ=[2.312746251838473], variance =[5.347064280796123]
[ Info: Iteration #1, current min val: 53.19981955218784
[ Info: Acquisition optimized, new candidate point: [4.18076462679124, -2.1829760306998867]
[ Info: Iteration #2, current min val: 22.24153811290866
[ Info: Acquisition optimized, new candidate point: [3.0321160770020845, 5.027404087232351]
[ Info: Iteration #3, current min val: 22.24153811290866
[ Info: Acquisition optimized, new candidate point: [3.952569491877039, 2.629550281343957]
[ Info: Iteration #4, current min val: 22.24153811290866
[ Info: Acquisition optimized, new candidate point: [-3.0136775036564485, 4.696040816128977]
[ Info: Iteration #5, current min val: 22.24153811290866
[ Info: Acquisition optimized, new candidate point: [-4.28861949486747, 2.729584623169842]
[ Info: Iteration #6, current min val: 22.24153811290866
[ Info: Acquisition optimized, new candidate point: [-2.771587342271167, 3.187143209191088]
[ Info: Iteration #7, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [-3.2548222747682574, -0.09378667781673782]
[ Info: Iteration #8, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [-4.5733267448053905, -3.3787850473215966]
[ Info: Iteration #9, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [-3.484647363806727, -3.0977896318607785]
[ Info: Iteration #10, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [-3.890423123497139, -5.999999999999999]
[ Info: Optimizing GP hyperparameters at iteration 11...
[ Info: New parameters: ℓ=[3.2704286252201618], variance =[171.6490951668017]
[ Info: Iteration #11, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [5.999999999999999, -5.999999999999971]
[ Info: Iteration #12, current min val: 0.16642648501143498
[ Info: Acquisition optimized, new candidate point: [3.5476248295037345, -1.819980796193849]
[ Info: Iteration #13, current min val: 0.07452732596827287
[ Info: Acquisition optimized, new candidate point: [2.9193776754828304, 2.208706998095649]
[ Info: Iteration #14, current min val: 0.07452732596827287
[ Info: Acquisition optimized, new candidate point: [3.0576343909603922, 1.8574914179217212]
[ Info: Iteration #15, current min val: 0.07452732596827287
[ Info: Acquisition optimized, new candidate point: [-3.786046627434117, -3.3096051920252405]
[ Info: Iteration #16, current min val: 0.028638521699334656
[ Info: Acquisition optimized, new candidate point: [3.5843617933335996, -1.849515325118609]
[ Info: Iteration #17, current min val: 2.9173467877657977e-5
[ Info: Acquisition optimized, new candidate point: [-5.9034, 5.9298]
[ Info: Iteration #18, current min val: 2.9173467877657977e-5
[ Info: Acquisition optimized, new candidate point: [-5.9346, -5.7918]
[ Info: Iteration #19, current min val: 2.9173467877657977e-5
[ Info: Acquisition optimized, new candidate point: [5.968200000000001, 5.857800000000001]
[ Info: Iteration #20, current min val: 2.9173467877657977e-5
[ Info: Acquisition optimized, new candidate point: [-2.8157999999999994, 3.1470000000000002]</code></pre><h3 id="Results-2"><a class="docs-heading-anchor" href="#Results-2">Results</a><a class="docs-heading-anchor-permalink" href="#Results-2" title="Permalink"></a></h3><p>The optimization result is stored in <code>result_grad</code>. We can print the best found input and its corresponding function value.</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimal point (GradBO): [3.5843617933335996, -1.849515325118609]
Optimal value (GradBO): 2.9173467877657977e-5</code></pre><h3 id="Plotting-of-running-minimum-over-iterations-2"><a class="docs-heading-anchor" href="#Plotting-of-running-minimum-over-iterations-2">Plotting of running minimum over iterations</a><a class="docs-heading-anchor-permalink" href="#Plotting-of-running-minimum-over-iterations-2" title="Permalink"></a></h3><p>The running minimum is the best function value found up to each iteration. Since each evaluation provides both a function value and a 2D gradient, we duplicate the running minimum values 3x to reflect the number of function evaluations.</p><img src="ce4058b4.svg" alt="Example block output"/><p>We observe that the gradient information does not necessarily lead to a better optimisation path in terms of function evaluations.</p><h3 id="Plotting-the-surrogate-model"><a class="docs-heading-anchor" href="#Plotting-the-surrogate-model">Plotting the surrogate model</a><a id="Plotting-the-surrogate-model-1"></a><a class="docs-heading-anchor-permalink" href="#Plotting-the-surrogate-model" title="Permalink"></a></h3><p>We can visualize the surrogate model&#39;s mean and uncertainty along with the true function and the evaluated</p><img src="d2aeb34e.svg" alt="Example block output"/><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../1D_BO/">« 1D Bayesian Optimisation</a><a class="docs-footer-nextpage" href="../acq_funcs_comparison/">1D Acquisition Function Comparison »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.0 on <span class="colophon-date" title="Wednesday 19 November 2025 11:47">Wednesday 19 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
