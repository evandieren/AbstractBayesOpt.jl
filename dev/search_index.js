var documenterSearchIndex = {"docs":
[{"location":"tutorials/acq_funcs_comparison/#AbstractBayesOpt-Tutorial:-Acquisition-Functions-Comparison-with-gradient-enhanced-GPs","page":"1D Acquisition Function Comparison","title":"AbstractBayesOpt Tutorial: Acquisition Functions Comparison with gradient-enhanced GPs","text":"","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Setup","page":"1D Acquisition Function Comparison","title":"Setup","text":"Loading the necessary packages.\n\nusing AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(555) # hide\nnothing # hide","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Define-the-objective-function","page":"1D Acquisition Function Comparison","title":"Define the objective function","text":"We will compare different acquisition functions on a 1D function with multiple local minima: f(x) = sin(x + 1) + sin(frac103(x + 1))\n\nf(x) = sin(x + 1) + sin((10.0 / 3.0) * (x + 1))\n∂f(x) = ForwardDiff.derivative(f, x)\nmin_f = −1.988699758534924 # hide\nf_∂f(x) = [f(x); ∂f(x)];\n\nd = 1\nlower = [-10.0]\nupper = [10.0]\ndomain = ContinuousDomain(lower, upper)\n\nplot_domain = lower[1]:0.01:upper[1] # hide\nys = f.(plot_domain) # hide\n\nplot( # hide\n    plot_domain, # hide\n    ys; # hide\n    xlim=(lower[1], upper[1]), # hide\n    label=\"f(x)\", # hide\n    xlabel=\"x\", # hide\n    ylabel=\"f(x)\", # hide\n    title=\"Objective Function: sin(x+1) + sin(10(x+1)/3)\", # hide\n    legend=:outertopright, # hide\n) # hide\n\napprox_min = [-8.9981; 9.8514] # hide\nf_min = f.(approx_min) # hide\nscatter!([approx_min], [f_min]; label=\"Global minima\", color=:red, markersize=5) # hide","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Initial-Training-Data","page":"1D Acquisition Function Comparison","title":"Initial Training Data","text":"We will use a gradient-enhanced Gaussian Process (GradientGP) with a Matérn 5/2 kernel. We add a small noise variance for numerical stability.\n\nσ² = 1e-12;\nnothing #hide\n\nGenerate initial training data using Sobol sampling for better space coverage\n\nn_train = 5\nx_train = vec(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\ny_train = f_∂f.(x_train)\n\nSetup the gradient-enhanced GP model, using in-house ApproxMatern52Kernel for AD compatibility.\n\nmodel = GradientGP(ApproxMatern52Kernel(), d+1, σ²)","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Acquisition-Functions-Setup","page":"1D Acquisition Function Comparison","title":"Acquisition Functions Setup","text":"We will compare five different acquisition functions:\n\nExpected Improvement (EI): Balances exploitation and exploration by considering both the magnitude and probability of improvement (see ExpectedImprovement)\nProbability of Improvement (PI): Focuses on the probability of finding a better point (see ProbabilityImprovement)\nUpper Confidence Bound (UCB): Uses lower bound estimates to guide exploration (see UpperConfidenceBound)\nUCB + Gradient UCB Ensemble: Combines standard UCB with gradient norm information (see GradientNormUCB and EnsembleAcquisition)\nEI + Gradient UCB Ensemble: Combines Expected Improvement with gradient norm information (see GradientNormUCB and EnsembleAcquisition)\n\nWe show below the function to setup the acquisition functions, and run the tests. You can skip to the results analysis and visualisation section if you want to see the outcomes directly.\n\nfunction setup_acquisition_functions(y_train)\n    best_y = minimum(first.(y_train))\n\n    ei_acq = ExpectedImprovement(0.0, best_y)\n\n    pi_acq = ProbabilityImprovement(0.0, best_y)\n\n    ucb_acq = UpperConfidenceBound(1.96)\n\n    ucb_acq_for_ensemble = UpperConfidenceBound(1.96)\n    grad_ucb_acq = GradientNormUCB(1.5)\n    ensemble_ucb_grad = EnsembleAcquisition(\n        [0.9, 0.1], [ucb_acq_for_ensemble, grad_ucb_acq]\n    )\n\n    ei_for_ensemble = ExpectedImprovement(0.0, best_y)\n    grad_ucb_for_ensemble = GradientNormUCB(1.5)\n    ensemble_ei_grad = EnsembleAcquisition(\n        [0.9, 0.1], [ei_for_ensemble, grad_ucb_for_ensemble]\n    )\n\n    return [\n        (\"EI\", ei_acq),\n        (\"PI\", pi_acq),\n        (\"UCB\", ucb_acq),\n        (\"UCB+GradUCB\", ensemble_ucb_grad),\n        (\"EI+GradUCB\", ensemble_ei_grad),\n    ]\nend","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Running-the-Optimisation-Comparison","page":"1D Acquisition Function Comparison","title":"Running the Optimisation Comparison","text":"Now we will run Bayesian optimisation with each acquisition function and compare their performance.\n\nfunction run_comparison(n_iterations=30)\n    results = Dict{String,Any}()\n\n    for (name, acq_func) in setup_acquisition_functions(y_train)\n        @info \"\\n=== Running optimisation with $name ===\"\n\n        problem = BOStruct(\n            f_∂f,\n            acq_func,\n            model,\n            domain,\n            x_train,\n            y_train,\n            n_iterations,\n            0.0,  # Actual noise level (0.0 for noiseless)\n        )\n\n        result, _, _ = AbstractBayesOpt.optimize(problem);\n\n        xs = result.xs\n        ys = first.(result.ys_non_std)\n\n        optimal_point = xs[argmin(ys)]\n        optimal_value = minimum(ys)\n\n        @info \"Optimal point: $optimal_point\"\n        @info \"Optimal value: $optimal_value\"\n        @info \"Error from true minimum: $(abs(optimal_value - min_f))\"\n\n        running_min = accumulate(min, f.(xs));\n\n        results[name] = (\n            xs=xs,\n            ys=ys,\n            running_min=running_min,\n            optimal_point=optimal_point,\n            optimal_value=optimal_value,\n            error=abs(optimal_value - min_f),\n        )\n    end\n\n    return results\nend;\nnothing #hide","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Execute-the-comparison","page":"1D Acquisition Function Comparison","title":"Execute the comparison","text":"Let's run the optimisation with each acquisition function for 30 iterations.\n\n@info \"Starting acquisition function comparison...\" # hide\nresults = run_comparison(30)","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Results-Analysis-and-Visualisation","page":"1D Acquisition Function Comparison","title":"Results Analysis and Visualisation","text":"We will create a convergence plot showing how each acquisition function performs over time. The plot shows the error relative to the true minimum on a logarithmic scale.\n\nfunction plot_convergence(results) # hide\n    p = plot(; # hide\n        title=\"Acquisition Function Comparison (1D GradBO)\", # hide\n        xlabel=\"Function evaluations\", # hide\n        yaxis=:log, # hide\n        legend=:bottomleft, # hide\n        linewidth=2, # hide\n    ) # hide\n\n    colors = [:blue, :red, :green, :orange, :purple] # hide\n\n    for (i, (name, result)) in enumerate(results) # hide\n        running_min_extended = collect( # hide\n            Iterators.flatten(fill(x, 2) for x in result.running_min), # hide\n        ) # hide\n        errors = max.(running_min_extended .- min_f, 1e-16)  # Avoid log(0) # hide\n\n        plot!( # hide\n            p, # hide\n            (2 * n_train):length(errors), # hide\n            errors[(2 * n_train):end]; # hide\n            label=name, # hide\n            color=colors[i], # hide\n            alpha=0.8, # hide\n        ) # hide\n    end # hide\n\n    vspan!(p, [1, 2*n_train]; color=:gray, alpha=0.2, label=\"Initial data\") # hide\n\n    return p # hide\nend; # hide\nnothing #hide","category":"section"},{"location":"tutorials/acq_funcs_comparison/#Create-and-display-the-convergence-plot","page":"1D Acquisition Function Comparison","title":"Create and display the convergence plot","text":"This plot shows how quickly each acquisition function converges to the global minimum. The ensemble methods that combine multiple acquisition functions often show improved performance.\n\nconv_plot = plot_convergence(results) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#AbstractBayesOpt-Tutorial:-NLML-Landscape-Visualisation","page":"Log Likelihood Landscape Visualisation","title":"AbstractBayesOpt Tutorial: NLML Landscape Visualisation","text":"This tutorial shows how to visualise the negative log marginal likelihood (NLML) landscape for Gaussian Process models. The NLML landscape shows how the model likelihood changes as we vary the kernel hyperparameters. Understanding this landscape is crucial for:\n\nChoosing appropriate hyperparameter optimisation strategies\nUnderstanding why some configurations converge faster than others\nIdentifying potential issues like local minima or ill-conditioned regions","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Setup","page":"Log Likelihood Landscape Visualisation","title":"Setup","text":"Loading the necessary packages.\n\nusing AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(42) # hide\nnothing # hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Define-the-objective-function","page":"Log Likelihood Landscape Visualisation","title":"Define the objective function","text":"We'll use the Himmelblau function again, as it provides a good test case with complex structure.\n\nhimmelblau(x::AbstractVector) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n∇himmelblau(x::AbstractVector) = ForwardDiff.gradient(himmelblau, x)\nf_val_grad(x::AbstractVector) = [himmelblau(x); ∇himmelblau(x)];\nnothing #hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Problem-Setup-and-Training-Data","page":"Log Likelihood Landscape Visualisation","title":"Problem Setup and Training Data","text":"d = 2\nlower = [-4.0, -4.0]\nupper = [4.0, 4.0]\ndomain = ContinuousDomain(lower, upper)\nσ² = 1e-6\n\nGenerate training data using Sobol sampling for better coverage\n\nn_train = 75\nx_train = [\n    collect(col) for\n    col in eachcol(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\n]\n\nEvaluate function at training points for both model types\n\ny_train_standard = [himmelblau(x) for x in x_train]  # Standard GP: only function values\ny_train_gradient = f_val_grad.(x_train);    # Gradient GP: function values + gradients\nnothing #hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Setup-Gaussian-Process-Models","page":"Log Likelihood Landscape Visualisation","title":"Setup Gaussian Process Models","text":"We'll create both standard and gradient-enhanced GP models using the same kernel type but configured for their respective data structures.\n\nkernel = ApproxMatern52Kernel()\n\nstandard_model = StandardGP(kernel, σ²)\ngradient_model = GradientGP(kernel, d+1, σ²)\n\nPrepare data for NLML computation (this is done under the hood in AbstractBayesOpt.jl)\n\nStandard GP data structure\n\nx_standard = x_train;\ny_standard = reduce(vcat, y_train_standard);\nnothing #hide\n\nGradient GP data structure\n\nx_gradient = KernelFunctions.MOInputIsotopicByOutputs(x_train, d+1);\ny_gradient = vec(permutedims(reduce(hcat, y_train_gradient)));\n\nprintln(\"Data shapes for NLML computation:\") # hide\nprintln(\"  Standard GP: x=$(length(x_standard)), y=$(length(y_standard))\") # hide\nprintln(\"  Gradient GP: x=$(length(x_gradient)), y=$(length(y_gradient))\") # hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Define-Parameter-Ranges-for-NLML-Landscape","page":"Log Likelihood Landscape Visualisation","title":"Define Parameter Ranges for NLML Landscape","text":"We will create a grid of hyperparameter values to evaluate the NLML landscape. The parameters we will vary are:\n\nLength scale: Controls how quickly the function varies spatially\nScale parameter: Controls the overall magnitude of function variations\n\nlog_lengthscale_range = range(log(1e-3), log(1e3); length=100) # hide\nlog_scale_range = range(log(1e-3), log(1e6); length=100) # hide\n\nprintln(\"Parameter ranges:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(exp(log_lengthscale_range[1]), digits=3)) to $(round(exp(log_lengthscale_range[end]), digits=3))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(exp(log_scale_range[1]), digits=3)) to $(round(exp(log_scale_range[end]), digits=3))\", # hide\n) # hide\n\nfunction compute_nlml_landscape( # hide\n    model,\n    x_data,\n    y_data,\n    log_ls_range,\n    log_scale_range,\n    model_name, # hide\n) # hide\n    @info \"Computing NLML landscape for $model_name...\" # hide\n\n    nlml_values = zeros(length(log_ls_range), length(log_scale_range)) # hide\n\n    total_combinations = length(log_ls_range) * length(log_scale_range) # hide\n    completed = 0 # hide\n\n    for (i, log_ls) in enumerate(log_ls_range) # hide\n        for (j, log_scale) in enumerate(log_scale_range) # hide\n            try # hide\n                params = [log_ls, log_scale] # hide\n                nlml_val = AbstractBayesOpt.nlml(model, params, x_data, y_data) # hide\n                nlml_values[i, j] = nlml_val # hide\n\n                if !isfinite(nlml_val) || nlml_val > 1e9 # hide\n                    @warn \"Warning: NLML value out of bounds at (log_ls=$(round(log_ls, digits=2)), log_scale=$(round(log_scale, digits=2))): $nlml_val\" # hide\n                    nlml_values[i, j] = 1e9 # hide\n                end # hide\n            catch e # hide\n                nlml_values[i, j] = 1e9 # hide\n            end # hide\n\n            completed += 1 # hide\n            if completed % 500 == 0 # hide\n                progress = round(100 * completed / total_combinations; digits=1) # hide\n                @info \"  Progress: $progress% ($completed/$total_combinations)\" # hide\n            end # hide\n        end # hide\n    end # hide\n\n    @info \"  Completed NLML landscape computation for $model_name\" # hide\n    @info \"  NLML range: $(round(minimum(nlml_values), digits=2)) to $(round(maximum(nlml_values), digits=2))\" # hide\n\n    return nlml_values # hide\nend # hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Compute-landscapes-for-both-models","page":"Log Likelihood Landscape Visualisation","title":"Compute landscapes for both models","text":"This computation may take several minutes depending on the grid resolution. We're evaluating 10,000 parameter combinations for each model type.\n\nnlml_standard = compute_nlml_landscape(\n    standard_model,\n    x_standard,\n    y_standard,\n    log_lengthscale_range,\n    log_scale_range,\n    \"Standard GP\",\n)\n\nnlml_gradient = compute_nlml_landscape(\n    gradient_model,\n    x_gradient,\n    y_gradient,\n    log_lengthscale_range,\n    log_scale_range,\n    \"Gradient GP\",\n)\n\nThis provides a 100x100 grid of NLML values for each model type.","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#Optimal-Parameters","page":"Log Likelihood Landscape Visualisation","title":"Optimal Parameters","text":"We approximately provide the hyperparameter combinations that minimise the NLML for each model type. In AbstractBayesOpt.jl, we optimise the MLML using Optim.jl's BFGS method.\n\nfunction find_optimal_params(nlml_values, log_ls_range, log_scale_range) # hide\n    min_idx = argmin(nlml_values) # hide\n    i, j = Tuple(min_idx) # hide\n    opt_log_ls = log_ls_range[i] # hide\n    opt_log_scale = log_scale_range[j] # hide\n    opt_nlml = nlml_values[i, j] # hide\n\n    return ( # hide\n        log_lengthscale=opt_log_ls, # hide\n        log_scale=opt_log_scale, # hide\n        lengthscale=exp(opt_log_ls), # hide\n        scale=exp(opt_log_scale), # hide\n        nlml=opt_nlml, # hide\n    ) # hide\nend # hide\n\nopt_standard = find_optimal_params(nlml_standard, log_lengthscale_range, log_scale_range) # hide\nopt_gradient = find_optimal_params(nlml_gradient, log_lengthscale_range, log_scale_range) # hide\n\nprintln(\"\\nOptimal parameters found:\") # hide\nprintln(\"Standard GP:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(opt_standard.lengthscale, digits=3)) (log: $(round(opt_standard.log_lengthscale, digits=3)))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(opt_standard.scale, digits=3)) (log: $(round(opt_standard.log_scale, digits=3)))\", # hide\n) # hide\nprintln(\"  NLML: $(round(opt_standard.nlml, digits=3))\") # hide\n\nprintln(\"\\nGradient GP:\") # hide\nprintln( # hide\n    \"  Lengthscale: $(round(opt_gradient.lengthscale, digits=3)) (log: $(round(opt_gradient.log_lengthscale, digits=3)))\", # hide\n) # hide\nprintln( # hide\n    \"  Scale: $(round(opt_gradient.scale, digits=3)) (log: $(round(opt_gradient.log_scale, digits=3)))\", # hide\n) # hide\nprintln(\"  NLML: $(round(opt_gradient.nlml, digits=3))\") # hide\n\nfunction create_contour_plot( # hide\n    nlml_values,\n    log_ls_range,\n    log_scale_range,\n    title_str,\n    optimal_params=nothing, # hide\n) # hide\n    p = contourf( # hide\n        log_scale_range, # hide\n        log_ls_range, # hide\n        log.(nlml_values); # hide\n        title=title_str, # hide\n        xlabel=\"log Scale Parameter\", # hide\n        ylabel=\"log Lengthscale Parameter\", # hide\n        color=:coolwarm, # hide\n        fill=true, # hide\n        levels=50, # hide\n        aspect_ratio=:equal, # hide\n        size=(600, 500), # hide\n    ) # hide\n\n    if optimal_params !== nothing # hide\n        scatter!( # hide\n            p, # hide\n            [log.(optimal_params.scale)], # hide\n            [log.(optimal_params.lengthscale)]; # hide\n            color=:red, # hide\n            markersize=8, # hide\n            markershape=:star, # hide\n            label=\"Optimal\", # hide\n            legend=:bottomright,\n        ) # hide\n    end # hide\n\n    return p # hide\nend; # hide\nnothing #hide","category":"section"},{"location":"tutorials/nlml_landscape_visualization/#NLML-Landscape-Plots","page":"Log Likelihood Landscape Visualisation","title":"NLML Landscape Plots","text":"These contour plots show the NLML landscape for both model types. The star indicates the optimal hyperparameter combination found through the approximate minimisers over the 100x100 grid. Darker blue regions correspond to lower NLML values (better likelihood).\n\ncontour_standard = create_contour_plot( # hide\n    nlml_standard, # hide\n    log_lengthscale_range, # hide\n    log_scale_range, # hide\n    \"Standard GP log(NLML) Contours\", # hide\n    opt_standard, # hide\n) # hide\n\ncontour_gradient = create_contour_plot( # hide\n    nlml_gradient, # hide\n    log_lengthscale_range, # hide\n    log_scale_range, # hide\n    \"Gradient GP log(NLML) Contours\", # hide\n    opt_gradient, # hide\n) # hide\n\ncombined_contours = plot( # hide\n    contour_standard, # hide\n    contour_gradient; # hide\n    layout=(1, 2), # hide\n    size=(1200, 500), # hide\n    plot_title=\"NLML Contour Comparison: Standard vs Gradient GP\", # hide\n) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/2D_BO/#AbstractBayesOpt-Tutorial:-Basic-2D-Optimisation","page":"2D Bayesian Optimisation","title":"AbstractBayesOpt Tutorial: Basic 2D Optimisation","text":"","category":"section"},{"location":"tutorials/2D_BO/#Setup","page":"2D Bayesian Optimisation","title":"Setup","text":"Loading the necessary packages.\n\nusing AbstractBayesOpt\nusing AbstractGPs\nusing ForwardDiff\nusing Plots\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nusing Random # hide\nRandom.seed!(42) # hide\nnothing #hide","category":"section"},{"location":"tutorials/2D_BO/#Define-the-objective-function","page":"2D Bayesian Optimisation","title":"Define the objective function","text":"f(x) = (x[1]^2 + x[2] - 11)^2 + (x[1]+x[2]^2-7)^2\nmin_f = 0.0 # hide\nd = 2\ndomain = ContinuousDomain([-6.0, -6.0], [6.0, 6.0])\n\nresolution = 100 #hide\nX = range(domain.lower[1], domain.upper[1]; length=resolution) # hide\nY = range(domain.lower[2], domain.upper[2]; length=resolution) # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> f([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Target function : Himmelblau\", # hide\n    xlabel=\"x₁\", # hide\n    ylabel=\"x₂\", # hide\n) # hide\n\nx_mins = [[3.0, 2.0], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]] #hide\n\nScatter them on the contour plot\n\nscatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"Minima\", # hide\n    color=:red, # hide\n    markersize=5, # hide\n    legend=:bottomright, # hide\n) # hide","category":"section"},{"location":"tutorials/2D_BO/#Standard-GPs","page":"2D Bayesian Optimisation","title":"Standard GPs","text":"We'll use a standard Gaussian Process surrogate with a squared-exponential kernel. We add a small jitter term for numerical stability of 10^-9.\n\nnoise_var = 1e-9\nsurrogate = StandardGP(SqExponentialKernel(), noise_var)\n\nGenerate uniform random samples x_train\n\nn_train = 5\nx_train = [domain.lower .+ (domain.upper .- domain.lower) .* rand(d) for _ in 1:n_train]\n\ny_train = f.(x_train)","category":"section"},{"location":"tutorials/2D_BO/#Choose-an-acquisition-function","page":"2D Bayesian Optimisation","title":"Choose an acquisition function","text":"We'll use the Expected Improvement acquisition function with an exploration parameter ξ = 0.0.\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))","category":"section"},{"location":"tutorials/2D_BO/#Set-up-the-Bayesian-Optimisation-structure","page":"2D Bayesian Optimisation","title":"Set up the Bayesian Optimisation structure","text":"We use BOStruct to bundle all components needed for the optimization. Here, we set the number of iterations to 5 and the actual noise level to 0.0 (since our function is noiseless). We then run the optimize function to perform the Bayesian Optimisation.\n\nbo_struct = BOStruct(\n    f,\n    acq,\n    surrogate,\n    domain,\n    x_train,\n    y_train,\n    50,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\"\nresult, acq_list, standard_params = AbstractBayesOpt.optimize(\n    bo_struct; standardize=\"mean_only\"\n);\nnothing #hide","category":"section"},{"location":"tutorials/2D_BO/#Results","page":"2D Bayesian Optimisation","title":"Results","text":"The optimization result is stored in result. We can print the best found input and its corresponding function value.\n\nxs = result.xs # hide\nys = result.ys_non_std # hide\n\nprintln(\"Optimal point: \", xs[argmin(ys)]) # hide\nprintln(\"Optimal value: \", minimum(ys)) # hide","category":"section"},{"location":"tutorials/2D_BO/#Plotting-of-running-minimum-over-iterations","page":"2D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"The running minimum is the best function value found up to each iteration.\n\nrunning_min = accumulate(min, f.(xs)) # hide\n\np = Plots.plot( # hide\n    n_train:length(running_min), # hide\n    running_min[n_train:end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (2D BO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"BO\", # hide\n    xlims=(1, length(running_min)), # hide\n) # hide\nPlots.vspan!([1, n_train]; color=:blue, alpha=0.2, label=\"\") # hide","category":"section"},{"location":"tutorials/2D_BO/#Gradient-enhanced-GPs","page":"2D Bayesian Optimisation","title":"Gradient-enhanced GPs","text":"Now, let's see how to use gradient information to improve the optimization. We'll use the same function but now also provide its gradient. We define a new surrogate model that can handle gradient information, specifically a GradientGP.\n\ngrad_surrogate = GradientGP(SqExponentialKernel(), d + 1, noise_var)\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))\n\n∇f(x) = ForwardDiff.gradient(f, x)\nf_val_grad(x) = [f(x); ∇f(x)];\nnothing #hide\n\nGenerate value and gradients at random samples\n\ny_train_grad = f_val_grad.(x_train)\n\nSet up the Bayesian Optimisation structure\n\nbo_struct_grad = BOStruct(\n    f_val_grad,\n    acq,\n    grad_surrogate,\n    domain,\n    x_train,\n    y_train_grad,\n    20,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\" # hide\nresult_grad, acq_list_grad, standard_params_grad = AbstractBayesOpt.optimize(bo_struct_grad);\nnothing #hide","category":"section"},{"location":"tutorials/2D_BO/#Results-2","page":"2D Bayesian Optimisation","title":"Results","text":"The optimization result is stored in result_grad. We can print the best found input and its corresponding function value.\n\nxs_grad = result_grad.xs # hide\nys_grad = first.(result_grad.ys_non_std) # hide\n\nx_min_grad = xs_grad[argmin(ys_grad)] # hide\ny_min_grad = minimum(ys_grad) # hide\n\nprintln(\"Optimal point (GradBO): \", x_min_grad) # hide\nprintln(\"Optimal value (GradBO): \", y_min_grad) # hide","category":"section"},{"location":"tutorials/2D_BO/#Plotting-of-running-minimum-over-iterations-2","page":"2D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"The running minimum is the best function value found up to each iteration. Since each evaluation provides both a function value and a 2D gradient, we duplicate the running minimum values 3x to reflect the number of function evaluations.\n\nrunning_min_grad = accumulate(min, f.(xs_grad)); # hide\nrunning_min_grad = collect(Iterators.flatten(fill(x, 3) for x in (running_min_grad))) # hide\n\np = Plots.plot( # hide\n    (3 * n_train):length(running_min_grad), # hide\n    running_min_grad[(3 * n_train):end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (2D GradBO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"gradBO\", # hide\n    xlims=(1, length(running_min_grad)), # hide\n) # hide\nPlots.vspan!([1, 3 * n_train]; color=:blue, alpha=0.2, label=\"\") # hide\n\nWe observe that the gradient information does not necessarily lead to a better optimisation path in terms of function evaluations.","category":"section"},{"location":"tutorials/2D_BO/#Plotting-the-surrogate-model","page":"2D Bayesian Optimisation","title":"Plotting the surrogate model","text":"We can visualize the surrogate model's mean and uncertainty along with the true function and the evaluated\n\nzipped_grid = [ # hide\n    [x1, x2] for # hide\n    (x1, x2) in zip(vec(repeat(X', resolution, 1)), vec(repeat(Y, 1, resolution))) # hide\n] # hide\n\nμ, σ² = unstandardized_mean_and_var(result_grad.model, zipped_grid, standard_params_grad) # hide\nμ_function_grid = reshape(μ[:, 1], resolution, resolution) # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> f([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Target function : Himmelblau\", # hide\n) # hide\n\nscatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"\", # hide\n    color=:green, # hide\n    markershape=:diamond, # hide\n    markersize=5, # hide\n) # hide\n\np2 = contour( # hide\n    X, # hide\n    Y, # hide\n    μ_function_grid; # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Surrogate mean - GradBO\", # hide\n) # hide\nscatter!( # hide\n    p2, # hide\n    [x[1] for x in x_train], # hide\n    [x[2] for x in x_train]; # hide\n    label=\"\", # hide\n    color=:black, # hide\n    markershape=:x, # hide\n    markersize=5,# hide\n) # hide\n\nscatter!( # hide\n    p2, # hide\n    [x[1] for x in xs_grad[(n_train + 1):end]], # hide\n    [x[2] for x in xs_grad[(n_train + 1):end]]; # hide\n    label=\"\", # hide\n    color=:orange, # hide\n    markersize=5, # hide\n) # hide\nscatter!( # hide\n    p2, # hide\n    [x_min_grad[1]], # hide\n    [x_min_grad[2]]; # hide\n    label=\"\", # hide\n    color=:red, # hide\n    markershape=:star5, # hide\n    markersize=8, # hide\n) # hide\n\np_legend = plot(; legend=:bottom, grid=false, axis=false, ticks=false, legend_columns=4) # hide\n\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Training points\", # hide\n    color=:black, # hide\n    markershape=:x, # hide\n    markersize=5, # hide\n) # hide\nscatter!(p_legend, [NaN], [NaN]; label=\"Candidate points\", color=:orange, markersize=5) # hide\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Best candidate\", # hide\n    color=:red, # hide\n    markershape=:star5, # hide\n    markersize=8, # hide\n) # hide\nscatter!( # hide\n    p_legend, # hide\n    [NaN], # hide\n    [NaN]; # hide\n    label=\"Function minima\", # hide\n    color=:green, # hide\n    markershape=:diamond, # hide\n    markersize=5, # hide\n) # hide\n\np = plot(p1, p2) # hide\nfinalplot = plot(p, p_legend; layout=@layout([a{0.9h}; b{0.1h}]), size=(1000, 400)) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"reference/#Reference-(Public-API)","page":"Public API","title":"Reference (Public API)","text":"This section documents the exported functions, types, etc from AbstractBayesOpt.jl.\n\n","category":"section"},{"location":"reference/#Bayesian-Optimisation-loop","page":"Public API","title":"Bayesian Optimisation loop","text":"","category":"section"},{"location":"reference/#Abstract-Interface","page":"Public API","title":"Abstract Interface","text":"","category":"section"},{"location":"reference/#Surrogates","page":"Public API","title":"Surrogates","text":"","category":"section"},{"location":"reference/#Kernels","page":"Public API","title":"Kernels","text":"","category":"section"},{"location":"reference/#GradientGP-related-functions","page":"Public API","title":"GradientGP-related functions","text":"","category":"section"},{"location":"reference/#Acquisition-Functions","page":"Public API","title":"Acquisition Functions","text":"","category":"section"},{"location":"reference/#Domains","page":"Public API","title":"Domains","text":"","category":"section"},{"location":"reference/#Continuous-domain","page":"Public API","title":"Continuous domain","text":"","category":"section"},{"location":"reference/#AbstractBayesOpt.BOStruct","page":"Public API","title":"AbstractBayesOpt.BOStruct","text":"BOStruct{F,M<:AbstractSurrogate,A<:AbstractAcquisition,D<:AbstractDomain,X,Y,T}(\n    func::F,\n    acq::A,\n    model::M,\n    domain::D,\n    xs::Vector{X},\n    ys::Vector{Y},\n    ys_non_std::Vector{Y},\n    max_iter::Int,\n    iter::Int,\n    noise::T,\n    flag::Bool,\n)\n\nA structure to hold all components of the Bayesian Optimization problem.\n\nAttributes:\n\nfunc::F: The target function to be optimized.\nacq::A: The acquisition function guiding the optimization.\nmodel::M: The surrogate model (e.g., Gaussian Process).\ndomain::D: The domain over which to optimize.\nxs::Vector{X}: A vector of input training points.\nys::Vector{Y}: A vector of corresponding output training values.\nys_non_std::Vector{Y}: A vector of output training values before standardization.\nmax_iter::Int: Maximum number of iterations for the optimization.\niter::Int: Current iteration number.\nnoise::T: Noise level in the observations.\nflag::Bool: A flag to indicate if the optimization should stop due to issues like   ill-conditioning.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.optimize","page":"Public API","title":"AbstractBayesOpt.optimize","text":"optimize(\n    BO::BOStruct;\n    standardize::Union{String,Nothing}=\"mean_scale\",\n    hyper_params::Union{String,Nothing}=\"all\",\n    num_restarts_HP::Int=1,\n)\n\nThis function implements the EGO framework:     While some criterion is not met,         (1) optimize the acquisition function to obtain the new best candidate,         (2) query the target function f,         (3) update the GP and the overall optimization state.     returns best found solution.\n\nArguments:\n\nBO::BOStruct: The Bayesian Optimization structure.\nstandardize: Specifies how to standardize the outputs.\nIf \"mean_scale\", standardize by removing mean and scaling by std.\nIf \"scale_only\", only scale the outputs without centering (in case we set a non-zero mean function with empirical mean).\nIf \"mean_only\", only remove the mean without scaling.\nIf nothing, do not standardize the outputs.\nhyper_params: Specifies how to handle hyperparameters.\nIf \"all\", re-optimize hyperparameters every 10 iterations.\nIf \"lengthscaleonly\", only optimize the lengthscale.\nIf nothing, do not re-optimize hyperparameters.\nnum_restarts_HP::Int: Number of random restarts for hyperparameter optimization.\n\nreturns:\n\nBO::BOStruct: The updated Bayesian Optimization problem after optimization.\nacqf_list::Vector: List of acquisition function values at each iteration.\nstandard_params::Tuple: Tuple containing the mean and standard deviation used for standardization\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.AbstractAcquisition","page":"Public API","title":"AbstractBayesOpt.AbstractAcquisition","text":"AbstractAcquisition\n\nAbstract type for acquisition functions used in Bayesian optimization.\n\nConcrete implementation should subtype this and implement the following methods:\n\n(acq::AbstractAcquisition)(surrogate::AbstractSurrogate, x::AbstractVector):   Evaluate the acquisition function at point x using the surrogate model.    This should also work for a single real input x::Real if working in 1D, in which case it is treated as a one-dimensional input vector. via the abstract method defined below.\nupdate(acq::AbstractAcquisition, ys::AbstractVector, model::AbstractSurrogate):   Update the acquisition function with new observations ys and the current surrogate model.\nBase.copy(acq::AbstractAcquisition):   Create a copy of the acquisition function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.AbstractDomain","page":"Public API","title":"AbstractBayesOpt.AbstractDomain","text":"AbstractDomain\n\nAn abstract type for defining the domain over which the optimization is performed.\n\nConcrete implementations should subtype this and define the necessary properties:\n\nlower: The lower bounds of the domain.\nupper: The upper bounds of the domain.\n\nas well as creating its constructor.\n\nOther methods can be added as needed depending on the use case.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.AbstractSurrogate","page":"Public API","title":"AbstractBayesOpt.AbstractSurrogate","text":"AbstractSurrogate\n\nAbstract type for surrogate models used in Bayesian optimization.\n\nConcrete implementation should subtype this and implement the following methods:\n\nupdate(model::AbstractSurrogate, xs::AbstractVector, ys::AbstractVector):   Update the surrogate model with new data points xs and corresponding observations ys.\nposterior_mean(surrogate::AbstractSurrogate, x::AbstractVector):   Compute the posterior mean of the surrogate model at point x.\nposterior_var(surrogate::AbstractSurrogate, x::AbstractVector):   Compute the posterior variance of the surrogate model at point x.\nnlml(surrogate::AbstractSurrogate, params::AbstractVector, xs::AbstractVector, ys::AbstractVector):   Compute the negative log marginal likelihood of the surrogate model given hyperparameters params, input data xs, and observations ys.\n\nIf you wish to standardize the outputs, you can also implement:\n\nstd_y(model::AbstractSurrogate):   Get the standard deviation used for standardizing the outputs in the surrogate model.\nget_mean_std(model::AbstractSurrogate):   Get the mean and standard deviation used for standardizing the outputs in the surrogate model.\n\nOther methods can be added as needed depending on the use case, and we refer to the impelementations of StandardGP and GradientGP for examples.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.StandardGP","page":"Public API","title":"AbstractBayesOpt.StandardGP","text":"StandardGP{T}(gp::AbstractGPs.GP, noise_var::T, gpx::Union{Nothing,AbstractGPs.PosteriorGP}) <: AbstractSurrogate\n\nImplementation of the Abstract structures for the standard GP.\n\nAttributes:\n\ngp::AbstractGPs.GP: The underlying Gaussian Process model.\nnoise_var::T: The noise variance of the observations.\ngpx::Union{Nothing,AbstractGPs.PosteriorGP}: The posterior GP after conditioning on data, nothing if not conditioned yet.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.GradientGP","page":"Public API","title":"AbstractBayesOpt.GradientGP","text":"struct GradientGP{T, G<:AbstractGPs.GP} <: AbstractSurrogate\n\nGradient-enhanced Gaussian Process surrogate model.\n\nAttributes:\n\ngp::G: The underlying Gaussian Process model.\nnoise_var::T: The noise variance of the observations.\np::Int: The number of outputs (1 for function value + d for gradients).\ngpx::Union{Nothing, AbstractGPs.PosteriorGP}: The posterior GP after conditioning on data,   nothing if not conditioned yet.\n\nNote: gpx is the posterior GP after conditioning on data, nothing if not conditioned yet\n\nThis relies on MOGP from AbstractGPs.jl and KernelFunctions.jl.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.posterior_mean","page":"Public API","title":"AbstractBayesOpt.posterior_mean","text":"posterior_mean(model::StandardGP, x::X) where {X}\n\nCompute the posterior mean of the GP at a new input point.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::X: A new input point where the prediction is to be made.\n\nreturns:\n\nmean: The posterior mean prediction at the input point.\n\n\n\n\n\nposterior_mean(model::StandardGP, x::AbstractVector)\n\nCompute the posterior mean of the GP at set of new input points.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::AbstractVector: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean: The posterior mean predictions at the input points.\n\n\n\n\n\nposterior_mean(model::GradientGP, x)\n\nCompute the function mean predictions of the GP model at new input points.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean::Vector: The mean predictions (function value only)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_var","page":"Public API","title":"AbstractBayesOpt.posterior_var","text":"posterior_var(model::StandardGP, x::X) where {X}\n\nCompute the posterior variance of the GP at a new input point.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::X: A new input point where the prediction is to be made.\n\nreturns:\n\nvar: The posterior variance prediction at the input point.\n\n\n\n\n\nposterior_var(model::StandardGP, x::AbstractVector)\n\nCompute the posterior variance of the GP at set of new input points.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nx::AbstractVector: A vector of new input points where predictions are to be made\n\nreturns:\n\nvar: The posterior variance predictions at the input points.\n\n\n\n\n\nposterior_var(model::GradientGP, x)\n\nCompute the function variance predictions of the GP model at new input points.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nvar::Vector: The variance predictions (function value only)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.nlml","page":"Public API","title":"AbstractBayesOpt.nlml","text":"nlml(model::StandardGP, params, xs::AbstractVector, ys::AbstractVector)\n\nCompute the negative log marginal likelihood (NLML) of the GP model given hyperparameters.\n\nArguments:\n\nmodel::StandardGP: The GP model.\nparams: A vector containing the log lengthscale and log scale parameters.\nxs::AbstractVector: The input data points.\nys::AbstractVector: The observed function values.\n\nreturns:\n\nnlml::Float64: The negative log marginal likelihood of the model.\n\n\n\n\n\nnlml(model::GradientGP, params, xs::AbstractVector, ys::AbstractVector)\n\nCompute the negative log marginal likelihood (NLML) of the GP model given hyperparameters.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nparams: Parameters containing the log lengthscale and log scale.\nxs::AbstractVector: The input data points.\nys::AbstractVector: The observed function values and gradients.\n\nreturns:\n\nnlml::Float64: The negative log marginal likelihood of the model.\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.ApproxMatern52Kernel","page":"Public API","title":"AbstractBayesOpt.ApproxMatern52Kernel","text":"ApproxMatern52Kernel{M}(metric::M) <: KernelFunctions.SimpleKernel\n\nApproximate Matern 5/2 kernel using a second-order Taylor expansion around d=0.\n\nAttributes:\n\nmetric: The distance metric to be used, defaults to squared Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ApproxMatern72Kernel","page":"Public API","title":"AbstractBayesOpt.ApproxMatern72Kernel","text":"ApproxMatern72Kernel{M}(metric::M) <: KernelFunctions.SimpleKernel\n\nApproximate Matern 7/2 kernel using a second-order Taylor expansion around d=0.\n\nAttributes:\n\nmetric: The distance metric to be used, defaults to squared Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ADMatern52Kernel","page":"Public API","title":"AbstractBayesOpt.ADMatern52Kernel","text":"ADMatern52Kernel{M} <: KernelFunctions.SimpleKernel\n\nMatern 5/2 kernel with custom differentiation rules for gradient computations.\n\nAttributes:\n\nmetric: The distance metric to be used, defaults to squared Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ADMatern72Kernel","page":"Public API","title":"AbstractBayesOpt.ADMatern72Kernel","text":"ADMatern72Kernel{M} <: KernelFunctions.SimpleKernel\n\nMatern 7/2 kernel with custom differentiation rules for gradient computations.\n\nAttributes:\n\nmetric: The distance metric to be used, defaults to squared Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.gradConstMean","page":"Public API","title":"AbstractBayesOpt.gradConstMean","text":"gradConstMean{V}(c::V)\n\nCustom mean function for the GradientGP model. Returns a constant per-output mean across MO inputs (function value + gradients). The first element corresponds to the function value, the following ones to the gradient outputs.\n\nUse gradConstMean([μ; zeros(d)]) to set a constant prior mean μ for the function value and zero for the gradients.\n\nAttributes:\n\nc::V: A vector of constants for each output (function value + gradients).\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.gradKernel","page":"Public API","title":"AbstractBayesOpt.gradKernel","text":"gradKernel{K}(base_kernel::K) <: MOKernel\n\nCustom kernel function for the GradientGP model that handles both function values and gradients.\n\nArguments:\n\nbase_kernel::KernelFunctions.Kernel: The base kernel function to be used.\n\nreturns:\n\ngradKernel: An instance of the custom gradient kernel function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.posterior_grad_mean","page":"Public API","title":"AbstractBayesOpt.posterior_grad_mean","text":"posterior_grad_mean(model::GradientGP, x)\n\nCompute the mean predictions of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nmean::Vector: The mean predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_grad_var","page":"Public API","title":"AbstractBayesOpt.posterior_grad_var","text":"posterior_grad_var(model::GradientGP, x)\n\nCompute the variance predictions of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\nvar::Vector: The variance predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.posterior_grad_cov","page":"Public API","title":"AbstractBayesOpt.posterior_grad_cov","text":"posterior_grad_cov(model::GradientGP, x)\n\nCompute the covariance matrix of the GP model at new input points, including gradients.\n\nArguments:\n\nmodel::GradientGP: The GP model.\nx: A vector of new input points where predictions are to be made.\n\nreturns:\n\ncov::Matrix: The covariance matrix of the predictions\n\n\n\n\n\n","category":"function"},{"location":"reference/#AbstractBayesOpt.EnsembleAcquisition","page":"Public API","title":"AbstractBayesOpt.EnsembleAcquisition","text":"EnsembleAcquisition(weights::Vector{Float64}, acqs::Vector{AbstractAcquisition}) <: AbstractAcquisition\n\nAn ensemble acquisition function combines multiple acquisition functions, each weighted by a specified factor,\n\nAttributes:\n\nweights::Vector{Float64}: A vector of non-negative weights for each acquisition function. The weights are normalized to sum to 1.\nacquisitions::Vector{AbstractAcquisition}: A vector of acquisition functions to be combined.\n\nRemark: All weights must be non-negative.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ExpectedImprovement","page":"Public API","title":"AbstractBayesOpt.ExpectedImprovement","text":"ExpectedImprovement{Y}(ξ::Y, best_y::Y) <: AbstractAcquisition\n\nExpected Improvement acquisition function.\n\nAttributes:\n\nξ::Y: Exploration parameter\nbest_y::Y: Best observed objective value\n\nReferences: Jones et al., 1998\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.GradientNormUCB","page":"Public API","title":"AbstractBayesOpt.GradientNormUCB","text":"GradientNormUCB{Y}(β::Y) <: AbstractAcquisition\n\nAcquisition function implementing the Squared 2-norm of the gradient with Upper Confidence Bound (UCB) exploration strategy.\n\nAttributes:\n\nβ::Y: Exploration-exploitation balance parameter\n\nReferences:     Derived by Van Dieren, E. but open to previous references if existing.     Originally proposed by Makrygiorgos et al., 2023 but adapted to the squared 2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ProbabilityImprovement","page":"Public API","title":"AbstractBayesOpt.ProbabilityImprovement","text":"ProbabilityImprovement{Y}(ξ::Y, best_y::Y) <: AbstractAcquisition\n\nAttributes:\n\nξ::Y: Exploration parameter\nbest_y::Y: Best observed objective value\n\nReferences: Kushner, 1964\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.UpperConfidenceBound","page":"Public API","title":"AbstractBayesOpt.UpperConfidenceBound","text":"UpperConfidenceBound{Y}(β::Y) <: AbstractAcquisition\n\nUpper Confidence Bound (UCB) acquisition function.\n\nAttributes:\n\nβ::Y: Exploration-exploitation balance parameter\n\nReferences: Srinivas et al., 2012\n\n\n\n\n\n","category":"type"},{"location":"reference/#AbstractBayesOpt.ContinuousDomain","page":"Public API","title":"AbstractBayesOpt.ContinuousDomain","text":"ContinuousDomain(lower::Vector{Float64}, upper::Vector{Float64}, bounds::Vector{Tuple{Float64,Float64}}) <: AbstractDomain\n\nA concrete implementation of AbstractDomain for continuous domains.\n\nAttributes:\n\nlower::Vector{Float64}: The lower bounds of the domain.\nupper::Vector{Float64}: The upper bounds of the domain.\nbounds::Vector{Tuple{Float64,Float64}}: A vector of tuples representing the (lower, upper) bounds for each dimension.\n\nConstructor:\n\nContinuousDomain(lower::Vector{Float64}, upper::Vector{Float64}):   Creates a ContinuousDomain instance given lower and upper bounds.   Performs sanity checks to ensure the bounds are valid.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractBayesOpt.jl","page":"Home","title":"AbstractBayesOpt.jl","text":"AbstractBayesOpt.jl is a Julia library for Bayesian Optimisation (BO), which relies on abstract classes for surrogate models, acquisition functions and domain definitions.\n\nThe library is designed to solve minimisation problems of the form:\n\nmin_x in mathcalX f(x)\n\nwhere f mathcalX to mathbbR is the objective function, which can be expensive to evaluate, non-differentiable, or noisy. The optimisation domain mathcalX subseteq mathbbR^d can be continuous, bounded, and possibly multi-dimensional.\n\nThe library uses Bayesian Optimisation (BO) to iteratively propose evaluation points x in the domain by:\n\nModeling the objective function with a surrogate model (e.g., Gaussian Process).\nUsing an acquisition function to select the next query point that balances exploration and exploitation.\nUpdating the surrogate with new observations and repeating until a stopping criterion is met.","category":"section"},{"location":"#How-AbstractBayesOpt.jl-fits-in-the-Julia-ecosystem","page":"Home","title":"How AbstractBayesOpt.jl fits in the Julia ecosystem","text":"AbstractBayesOpt.jl provides a modular, abstract framework for Bayesian Optimisation in Julia. It defines three core abstractions: AbstractSurrogate, AbstractAcquisition, and AbstractDomain, as well as a standard optimisation loop, allowing users to plug in any surrogate model, acquisition function, or search space.\n\nUnlike traditional BO libraries that rely on a specific surrogate implementation (e.g. BayesianOptimization.jl using GaussianProcesses.jl), AbstractBayesOpt.jl is fully flexible. Users are free to use packages such as AbstractGPs.jl or GaussianProcesses.jl; in fact, our standard and gradient-enhanced GP implementations leverage AbstractGPs.jl and KernelFunctions.jl. We also mention the Surrogates.jl package, that implements a high level of BO using implemented surrogates (Kriging, Gradient-Enhanced Kriging, GPs from AbstractGPs.jl) \n\nIn short, AbstractBayesOpt.jl acts as a general \"glue\" layer, unifying the Julia BO ecosystem under a simple and extensible interface.","category":"section"},{"location":"#Abstract-Interfaces","page":"Home","title":"Abstract Interfaces","text":"We currently have three main abstract interfaces that work with our BO loop:\n\nAbstractAcquisition: Interface to implement for an acquisition function to be used in AbstractBayesOpt.jl.\nAbstractDomain: Interface to implement for the optimisation domain to be used in AbstractBayesOpt.jl.\nAbstractSurrogate: Interface to implement for a surrogate to be used in AbstractBayesOpt.jl.\n\nAbstractBayesOpt.jl defines the core abstractions for building Bayesian optimisation algorithms. To add a new surrogate model, acquisition function, or domain, implement the following interfaces.","category":"section"},{"location":"#Acquisition-Functions","page":"Home","title":"Acquisition Functions","text":"Subtype AbstractAcquisition and implement:\n\n(acq::AbstractAcquisition)(model::AbstractSurrogate, x::AbstractVector):   Evaluate the acquisition function at x. We view x as a set of observations, and hence return a vector when we query acq.\nupdate(acq::AbstractAcquisition, ys::AbstractVector, model::AbstractSurrogate):   Update acquisition state given new observations.\nBase.copy(acq::AbstractAcquisition):   Return a copy of the acquisition function.","category":"section"},{"location":"#Domains","page":"Home","title":"Domains","text":"Subtype AbstractDomain and implement:\n\nConcrete implementations should subtype this and define the necessary properties:\n\nlower: The lower bounds of the domain.\nupper: The upper bounds of the domain.\n\nas well as creating its constructor.\n\nConcrete implementations may add additional methods as needed, but these are the minimum required for compatibility with the BO loop. We note that we are using Optim.jl to solve the  acquisition function maximisation problem for now,and hence the lower and  upper bounds must be compatible with their optimisation interface, which might  limit quite a lot the type of usable domains.","category":"section"},{"location":"#Surrogates","page":"Home","title":"Surrogates","text":"Subtype AbstractSurrogate and implement:\n\nupdate(model::AbstractSurrogate, xs::AbstractVector, ys::AbstractVector):   Update the surrogate with new data (xs, ys).\nposterior_mean(model::AbstractSurrogate, x):   Return the posterior mean at points x.\nposterior_var(model::AbstractSurrogate, x):   Return the posterior variance at points x.\nnlml(model::AbstractSurrogate, params, xs::AbstractVector, ys::AbstractVector):   Compute the negative log marginal likelihood given hyperparameters and data.","category":"section"},{"location":"#What-is-currently-implemented?","page":"Home","title":"What is currently implemented?","text":"We list below the abstract subtypes currently implemented in AbstractBayesOpt.jl.","category":"section"},{"location":"#Surrogates-2","page":"Home","title":"Surrogates","text":"StandardGP: Gaussian Process surrogate model with standard mean and covariance functions.\nGradientGP: Gaussian Process surrogate model supporting gradient information.","category":"section"},{"location":"#Acquisition-functions","page":"Home","title":"Acquisition functions","text":"ExpectedImprovement: Standard expected improvement acquisition function for balancing exploration and exploitation.\nUpperConfidenceBound: Acquisition function using a confidence bound to guide optimisation.\nGradientNormUCB: Gradient-based variant of the Upper Confidence Bound acquisition function.\nProbabilityImprovement: Probability of improvement acquisition function.\nEnsembleAcquisition: Combines multiple acquisition functions into an ensemble to leverage complementary strategies.","category":"section"},{"location":"#Domains-2","page":"Home","title":"Domains","text":"ContinuousDomain: Represents a continuous optimisation domain, defining bounds and dimensionality for optimisation problems.","category":"section"},{"location":"tutorials/1D_BO/#AbstractBayesOpt-Tutorial:-1D-Bayesian-Optimisation","page":"1D Bayesian Optimisation","title":"AbstractBayesOpt Tutorial: 1D Bayesian Optimisation","text":"","category":"section"},{"location":"tutorials/1D_BO/#Setup","page":"1D Bayesian Optimisation","title":"Setup","text":"Loading the necessary packages.\n\nusing AbstractBayesOpt\nusing AbstractGPs\nusing ForwardDiff\nusing Plots\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nusing Random # hide\nRandom.seed!(42) # hide\nnothing # hide","category":"section"},{"location":"tutorials/1D_BO/#Define-the-objective-function","page":"1D Bayesian Optimisation","title":"Define the objective function","text":"We will optimise a simple 1D function: f(x) = (x-2)^2 + sin(3x)\n\nf(x) = (x - 2)^2 + sin(3x)\nmin_f = -0.8494048256167165 # hide\nd = 1\ndomain = ContinuousDomain([0.0], [5.0])\n\nplot_domain = domain.lower[1]:0.01:domain.upper[1] #hide\nys = f.(plot_domain) #hide\n\nplot(                                           #hide\n    plot_domain,                                #hide\n    ys;                                          #hide\n    xlim=(domain.lower[1], domain.upper[1]),    #hide\n    label=\"f(x)\",               #hide\n    xlabel=\"x\",                 #hide\n    ylabel=\"f(x)\",             #hide\n    legend=:outertopright,              #hide\n) #hide\n\nx_min = plot_domain[argmin(ys)] #hide\nscatter!([x_min], [minimum(ys)]; label=\"Minimum\", color=:red, markersize=5) #hide","category":"section"},{"location":"tutorials/1D_BO/#Standard-GPs","page":"1D Bayesian Optimisation","title":"Standard GPs","text":"We'll use a standard Gaussian Process surrogate with a Matérn 5/2 kernel. We add a small jitter term for numerical stability of 10^-12.\n\nnoise_var = 1e-12\nsurrogate = StandardGP(Matern52Kernel(), noise_var)\n\nGenerate uniform random samples x_train and evaluate the function at these points to get y_train.\n\nn_train = 5\nx_train = first.([\n    domain.lower .+ (domain.upper .- domain.lower) .* rand(d) for _ in 1:n_train\n])\n\ny_train = f.(x_train)","category":"section"},{"location":"tutorials/1D_BO/#Choose-an-acquisition-function","page":"1D Bayesian Optimisation","title":"Choose an acquisition function","text":"We'll use the Expected Improvement acquisition function with an exploration parameter ξ = 0.0.\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))","category":"section"},{"location":"tutorials/1D_BO/#Set-up-the-Bayesian-Optimisation-structure","page":"1D Bayesian Optimisation","title":"Set up the Bayesian Optimisation structure","text":"We use BOStruct to bundle all components needed for the optimisation. Here, we set the number of iterations to 5 and the actual noise level to 0.0 (since our function is noiseless). We then run the optimize function to perform the Bayesian optimisation.\n\nbo_struct = BOStruct(\n    f,\n    acq,\n    surrogate,\n    domain,\n    x_train,\n    y_train,\n    30,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian ...\"\nresult, acq_list, standard_params = AbstractBayesOpt.optimize(\n    bo_struct; standardize=\"mean_only\"\n);\nnothing #hide","category":"section"},{"location":"tutorials/1D_BO/#Results","page":"1D Bayesian Optimisation","title":"Results","text":"The  result is stored in result. We can print the best found input and its corresponding function value.\n\nxs = result.xs # hide\nys = result.ys_non_std # hide\n\nprintln(\"Optimal point: \", xs[argmin(ys)]) # hide\nprintln(\"Optimal value: \", minimum(ys)) # hide","category":"section"},{"location":"tutorials/1D_BO/#Plotting-of-running-minimum-over-iterations","page":"1D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"The running minimum is the best function value found up to each iteration.\n\nrunning_min = accumulate(min, f.(xs)) # hide\n\np = Plots.plot( # hide\n    n_train:length(running_min),  # hide\n    running_min[n_train:end] .- min_f;  # hide\n    yaxis=:log,  # hide\n    title=\"Error w.r.t true minimum (1D BO)\",  # hide\n    xlabel=\"Function evaluations\",  # hide\n    label=\"BO\",  # hide\n    xlims=(1, length(running_min)),  # hide\n) # hide\nPlots.vspan!([1, n_train]; color=:blue, alpha=0.2, label=\"training GP\") # hide","category":"section"},{"location":"tutorials/1D_BO/#Gradient-enhanced-GPs","page":"1D Bayesian Optimisation","title":"Gradient-enhanced GPs","text":"Now, let's see how to use gradient information to improve the optimisation. We'll use the same function but now also provide its gradient. We define a new surrogate model that can handle gradient information, specifically a GradientGP.\n\ngrad_surrogate = GradientGP(ApproxMatern52Kernel(), d + 1, noise_var)\n\nξ = 0.0\nacq = ExpectedImprovement(ξ, minimum(y_train))\n\n∂f(x) = ForwardDiff.derivative(f, x)\nf_∂f(x) = [f(x); ∂f(x)];\nnothing #hide\n\nGenerate value and gradients at random samples\n\ny_train_grad = f_∂f.(x_train)\n\nSet up the Bayesian Optimisation structure\n\nbo_struct_grad = BOStruct(\n    f_∂f,\n    acq,\n    grad_surrogate,\n    domain,\n    x_train,\n    y_train_grad,\n    10,  # number of iterations\n    0.0,  # Actual noise level (0.0 for noiseless)\n)\n\n@info \"Starting Bayesian Optimisation...\" # hide\nresult_grad, acq_list_grad, standard_params_grad = AbstractBayesOpt.optimize(\n    bo_struct_grad; standardize=\"mean_only\"\n);\nnothing #hide","category":"section"},{"location":"tutorials/1D_BO/#Results-2","page":"1D Bayesian Optimisation","title":"Results","text":"The  result is stored in result_grad. We can print the best found input and its corresponding function value.\n\nxs_grad = reduce(vcat, result_grad.xs) # hide\nys_grad = hcat(result_grad.ys_non_std...)[1, :] # hide\n\nprintln(\"Optimal point (GradBO): \", xs_grad[argmin(ys_grad)]) # hide\nprintln(\"Optimal value (GradBO): \", minimum(ys_grad)) # hide","category":"section"},{"location":"tutorials/1D_BO/#Plotting-of-running-minimum-over-iterations-2","page":"1D Bayesian Optimisation","title":"Plotting of running minimum over iterations","text":"The running minimum is the best function value found up to each iteration. Since each evaluation provides both a function value and a 1D gradient, we duplicate the running minimum values to reflect the number of function evaluations.\n\nrunning_min_grad = accumulate(min, f.(xs_grad)) # hide\nrunning_min_grad = collect(Iterators.flatten(fill(x, 2) for x in (running_min_grad))) # hide\n\np = Plots.plot( # hide\n    (2 * n_train):length(running_min_grad), # hide\n    running_min_grad[(2 * n_train):end] .- min_f; # hide\n    yaxis=:log, # hide\n    title=\"Error w.r.t true minimum (1D GradBO)\", # hide\n    xlabel=\"Function evaluations\", # hide\n    label=\"gradBO\", # hide\n    xlims=(1, length(running_min_grad)), # hide\n) # hide\nPlots.vspan!([1, 2 * n_train]; color=:blue, alpha=0.2, label=\"\") # hide","category":"section"},{"location":"tutorials/1D_BO/#Plotting-the-surrogate-model","page":"1D Bayesian Optimisation","title":"Plotting the surrogate model","text":"We can visualize the surrogate model's mean and uncertainty along with the true function and the evaluated\n\nplot_domain = collect(domain.lower[1]:0.01:domain.upper[1])\n\nplot_x = map(x -> [x], plot_domain)\nplot_x = prep_input(grad_surrogate, plot_x)\npost_mean, post_var = unstandardized_mean_and_var(\n    result_grad.model, plot_x, standard_params_grad\n)\n\npost_mean = reshape(post_mean, :, d + 1)[:, 1]\npost_var = reshape(post_var, :, d + 1)[:, 1]\npost_var[post_var .< 0] .= 0\n\nplot( #hide\n    plot_domain, # hide\n    f.(plot_domain); # hide\n    label=\"target function\", # hide\n    xlim=(domain.lower[1], domain.upper[1]), # hide\n    xlabel=\"x\", # hide\n    ylabel=\"y\", # hide\n    title=\"AbstractBayesOpt\", # hide\n    legend=:outertopright, # hide\n) # hide\nplot!( #hide\n    plot_domain, # hide\n    post_mean; # hide\n    label=\"gradGP\", # hide\n    ribbon=sqrt.(post_var), # hide\n    ribbon_scale=2, # hide\n    color=\"green\", # hide\n) # hide\nscatter!(xs_grad[1:n_train], ys_grad[1:n_train]; label=\"Train Data\") # hide\nscatter!(xs_grad[(n_train + 1):end], ys_grad[(n_train + 1):end]; label=\"Candidates\") # hide\nscatter!([xs_grad[argmin(ys_grad)]], [minimum(ys_grad)]; label=\"Best candidate\") # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/hyperparams_comparison/#AbstractBayesOpt-Tutorial:-Hyperparameter-Tuning-and-Standardisation-Comparison","page":"Hyperparameter & Standardisation Comparison","title":"AbstractBayesOpt Tutorial: Hyperparameter Tuning and Standardisation Comparison","text":"This tutorial presents the different options for hyperparameter optimisation and standardisation modes available in AbstractBayesOpt.jl. We will compare the performance of these configurations on gradient-enhanced GPs on the Himmelblau function","category":"section"},{"location":"tutorials/hyperparams_comparison/#Setup","page":"Hyperparameter & Standardisation Comparison","title":"Setup","text":"Loading the necessary packages.\n\nusing AbstractBayesOpt\nusing AbstractGPs\nusing Plots\nusing ForwardDiff\nusing QuasiMonteCarlo\nusing Random\n\ndefault(; legend=:outertopright, size=(700, 400)) # hide\n\nRandom.seed!(42) # hide\nnothing # hide","category":"section"},{"location":"tutorials/hyperparams_comparison/#Define-the-objective-function","page":"Hyperparameter & Standardisation Comparison","title":"Define the objective function","text":"We will use the Himmelblau function, a well-known multi-modal test function with four global minima. The function is defined as: f(x_1 x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2\n\nhimmelblau(x::AbstractVector) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n∇himmelblau(x) = ForwardDiff.gradient(himmelblau, x)\n\nCombined function that returns both value and gradient for our gradient-enhanced GP\n\nf_val_grad(x) = [himmelblau(x); ∇himmelblau(x)]\n\nglobal_min = 0.0 # hide\n\nd = 2\nlower = [-6.0, -6.0]\nupper = [6.0, 6.0]\ndomain = ContinuousDomain(lower, upper)\n\nresolution = 100 # hide\nX = range(lower[1], upper[1]; length=resolution) # hide\nY = range(lower[2], upper[2]; length=resolution) # hide\nx_mins = [[3.0, 2.0], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]] # hide\n\np1 = contour( # hide\n    X, # hide\n    Y, # hide\n    (x, y) -> himmelblau([x, y]); # hide\n    fill=true, # hide\n    levels=50, # hide\n    c=:coolwarm, # hide\n    title=\"Himmelblau Function\", # hide\n    xlabel=\"x₁\", # hide\n    ylabel=\"x₂\", # hide\n) # hide\n\nscatter!( # hide\n    [p[1] for p in x_mins], # hide\n    [p[2] for p in x_mins]; # hide\n    label=\"Global minima\", # hide\n    color=:red, # hide\n    markersize=5, # hide\n    legend=:bottomright, # hide\n) # hide","category":"section"},{"location":"tutorials/hyperparams_comparison/#Initial-Training-Data-and-Model-Setup","page":"Hyperparameter & Standardisation Comparison","title":"Initial Training Data and Model Setup","text":"We'll use a gradient-enhanced Gaussian Process with an approximate Matérn 5/2 kernel. For better space coverage, we generate initial training data using Sobol sampling.\n\nσ² = 1e-12\n\nn_train = 8\nx_train = [\n    collect(col) for\n    col in eachcol(QuasiMonteCarlo.sample(n_train, lower, upper, SobolSample()))\n]\n\nEvaluate function and gradients at training points\n\ny_train = f_val_grad.(x_train)\n\nSetup the base model that we'll use across all configurations\n\nbase_model = GradientGP(ApproxMatern52Kernel(), d+1, σ²)","category":"section"},{"location":"tutorials/hyperparams_comparison/#Configuration-Setup","page":"Hyperparameter & Standardisation Comparison","title":"Configuration Setup","text":"We'll compare different combinations of hyperparameter optimisation strategies and standardisation modes. This comprehensive comparison will help us understand how these settings affect optimisation performance.","category":"section"},{"location":"tutorials/hyperparams_comparison/#Hyperparameter-Optimisation-Strategies:","page":"Hyperparameter & Standardisation Comparison","title":"Hyperparameter Optimisation Strategies:","text":"\"all\": Optimise all kernel hyperparameters (length scales, signal variance, etc.)\n\"lengthscaleonly\": Only optimise the length scale parameters\nnothing: Use fixed hyperparameters (no optimisation)","category":"section"},{"location":"tutorials/hyperparams_comparison/#Standardisation-Modes:","page":"Hyperparameter & Standardisation Comparison","title":"Standardisation Modes:","text":"\"mean_scale\": Remove empirical mean and scale by standard deviation (default)\n\"scale_only\": Only scale by standard deviation\n\"mean_only\": Only remove empirical mean, no scaling\nnothing: No standardisation applied\n\ntest_configs = [ # hide\n    (\"HP:all + MeanScale\", \"all\", \"mean_scale\"), # hide\n    (\"HP:all + ScaleOnly\", \"all\", \"scale_only\"), # hide\n    (\"HP:all + MeanOnly\", \"all\", \"mean_only\"), # hide\n    (\"HP:all + NoStd\", \"all\", nothing), # hide\n    (\"HP:length + MeanScale\", \"length_scale_only\", \"mean_scale\"), # hide\n    (\"HP:length + ScaleOnly\", \"length_scale_only\", \"scale_only\"), # hide\n    (\"HP:length + MeanOnly\", \"length_scale_only\", \"mean_only\"), # hide\n    (\"HP:length + NoStd\", \"length_scale_only\", nothing), # hide\n    (\"HP:none + MeanScale\", nothing, \"mean_scale\"), # hide\n    (\"HP:none + ScaleOnly\", nothing, \"scale_only\"), # hide\n    (\"HP:none + MeanOnly\", nothing, \"mean_only\"), # hide\n    (\"HP:none + NoStd\", nothing, nothing), # hide\n]# hide","category":"section"},{"location":"tutorials/hyperparams_comparison/#Running-the-Optimisation-Comparison","page":"Hyperparameter & Standardisation Comparison","title":"Running the Optimisation Comparison","text":"We will run Bayesian optimisation with each configuration and collect performance metrics.\n\nfunction run_comparison(n_iterations) # hide\n    results = Dict{String,NamedTuple}() # hide\n\n    for (config_name, hyper_params, standardise_mode) in test_configs # hide\n        model = deepcopy(base_model) # hide\n\n        best_y = minimum(first.(y_train)) # hide\n        acq_func = ExpectedImprovement(0.0, best_y) # hide\n\n        problem = BOStruct( # hide\n            f_val_grad, # hide\n            acq_func, # hide\n            model, # hide\n            domain, # hide\n            x_train, # hide\n            y_train, # hide\n            n_iterations, # hide\n            0.0,   # hide\n        ) # hide\n\n        start_time = time() # hide\n\n        try # hide\n            result, _, standard_params = AbstractBayesOpt.optimize( # hide\n                problem;\n                hyper_params=hyper_params,\n                standardize=standardise_mode, # hide\n            ) # hide\n\n            end_time = time() # hide\n            elapsed_time = end_time - start_time # hide\n\n            xs = result.xs # hide\n            ys_values = first.(result.ys_non_std) # hide\n\n            optimal_idx = argmin(ys_values) # hide\n            optimal_point = xs[optimal_idx] # hide\n            optimal_value = minimum(ys_values) # hide\n\n            all_evals = himmelblau.(xs) # hide\n            running_min = accumulate(min, all_evals) # hide\n\n            errors = max.(running_min .- global_min, 1e-16) # hide\n\n            results[config_name] = ( # hide\n                xs=xs, # hide\n                ys_values=ys_values, # hide\n                running_min=running_min, # hide\n                errors=errors, # hide\n                optimal_point=optimal_point, # hide\n                optimal_value=optimal_value, # hide\n                error_from_global=abs(optimal_value - global_min), # hide\n                elapsed_time=elapsed_time, # hide\n                hyper_params=hyper_params, # hide\n                standardize=standardise_mode, # hide\n                standard_params=standard_params, # hide\n                n_evaluations=length(xs), # hide\n            ) # hide\n\n        catch e # hide\n            @warn \"ERROR in configuration $config_name: $e\" # hide\n            results[config_name] = ( # hide\n                error_from_global=Inf, # hide\n                elapsed_time=Inf, # hide\n                hyper_params=hyper_params, # hide\n                standardize=standardise_mode, # hide\n                n_evaluations=0, # hide\n            ) # hide\n        end # hide\n    end # hide\n\n    return results # hide\nend; # hide\nnothing #hide","category":"section"},{"location":"tutorials/hyperparams_comparison/#Execute-the-comparison","page":"Hyperparameter & Standardisation Comparison","title":"Execute the comparison","text":"Let's run the optimisation with all 12 different configurations. This will take some time as we're testing various combinations of hyperparameter optimisation and standardisation settings.\n\n@info \"Starting comparison...\" # hide\nresults = run_comparison(30)","category":"section"},{"location":"tutorials/hyperparams_comparison/#Results-Analysis-and-Visualisation","page":"Hyperparameter & Standardisation Comparison","title":"Results Analysis and Visualisation","text":"function plot_convergence_comparison(results) # hide\n    p = plot(; # hide\n        title=\"Himmelblau Optimisation: Hyperparameter & Standardisation Comparison\", # hide\n        xlabel=\"Number of iterations\", # hide\n        ylabel=\"Error from global minimum\", # hide\n        yaxis=:log, # hide\n        legend=:bottomleft, # hide\n        linewidth=2, # hide\n        size=(1200, 800), # hide\n    ) # hide\n\n    colors = [ # hide\n        :blue, # hide\n        :lightblue, # hide\n        :cyan, # hide\n        :gray, # hide\n        :red, # hide\n        :pink, # hide\n        :orange, # hide\n        :brown, # hide\n        :green, # hide\n        :lightgreen, # hide\n        :yellow, # hide\n        :purple, # hide\n    ] # hide\n    styles = [ # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n        :solid, # hide\n        :dash, # hide\n        :dot, # hide\n        :dashdot, # hide\n    ] # hide\n\n    config_names = [ # hide\n        \"HP:all + MeanScale\", # hide\n        \"HP:all + ScaleOnly\", # hide\n        \"HP:all + MeanOnly\", # hide\n        \"HP:all + NoStd\", # hide\n        \"HP:length + MeanScale\", # hide\n        \"HP:length + ScaleOnly\", # hide\n        \"HP:length + MeanOnly\", # hide\n        \"HP:length + NoStd\", # hide\n        \"HP:none + MeanScale\", # hide\n        \"HP:none + ScaleOnly\", # hide\n        \"HP:none + MeanOnly\", # hide\n        \"HP:none + NoStd\", # hide\n    ] # hide\n\n    for (i, config_name) in enumerate(config_names) # hide\n        if haskey(results, config_name) && haskey(results[config_name], :errors) # hide\n            result = results[config_name] # hide\n            plot!( # hide\n                p, # hide\n                1:length(result.errors), # hide\n                result.errors; # hide\n                label=config_name, # hide\n                color=colors[i], # hide\n                linestyle=styles[i], # hide\n            ) # hide\n        end # hide\n    end # hide\n\n    vspan!(p, [1, n_train]; color=:gray, alpha=0.2, label=\"Initial data\") # hide\n\n    return p # hide\nend; #hide\nnothing #hide","category":"section"},{"location":"tutorials/hyperparams_comparison/#Create-and-display-the-convergence-plot","page":"Hyperparameter & Standardisation Comparison","title":"Create and display the convergence plot","text":"This comprehensive plot shows the optimisation performance of all 12 configurations. Each line represents a different combination of hyperparameter optimisation and standardisation.\n\nconv_plot = plot_convergence_comparison(results) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
